{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b4550b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthiasherp/anaconda3/envs/mctm_pytorch/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# GTM Package including GTM and the plot functions\n",
    "from gtm import *\n",
    "# Helper functions to create simulation data and analyze it\n",
    "from demos.pyvinecopulib_simulation_helpers import *\n",
    "# Sample Copulas Package\n",
    "import pyvinecopulib as pv\n",
    "import numpy as np\n",
    "# Other Stuff\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from demos.dataset_helpers import Generic_Dataset\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc123abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed17c60",
   "metadata": {},
   "source": [
    "### 1. Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fc86d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "rvine_structure = pv.RVineStructure.simulate(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34d90e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 10  # dimension\n",
    "pair_copulas = sample_random_pair_copulas(D,Independence_tree=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dcaf6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "vine_model = pv.Vinecop.from_structure(structure=rvine_structure, pair_copulas=pair_copulas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cb304e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tree</th>\n",
       "      <th>edge</th>\n",
       "      <th>conditioned variables</th>\n",
       "      <th>conditioning variables</th>\n",
       "      <th>var_types</th>\n",
       "      <th>family</th>\n",
       "      <th>rotation</th>\n",
       "      <th>parameters</th>\n",
       "      <th>df</th>\n",
       "      <th>tau</th>\n",
       "      <th>conditioned variable 1</th>\n",
       "      <th>conditioned variable 2</th>\n",
       "      <th>var_row</th>\n",
       "      <th>var_col</th>\n",
       "      <th>dependence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3  2</td>\n",
       "      <td></td>\n",
       "      <td>c, c</td>\n",
       "      <td>Joe</td>\n",
       "      <td>270.0</td>\n",
       "      <td>5.44</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9  8</td>\n",
       "      <td></td>\n",
       "      <td>c, c</td>\n",
       "      <td>Student</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.42, 2.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.28</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4  2</td>\n",
       "      <td></td>\n",
       "      <td>c, c</td>\n",
       "      <td>Gaussian</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1  5</td>\n",
       "      <td></td>\n",
       "      <td>c, c</td>\n",
       "      <td>Joe</td>\n",
       "      <td>180.0</td>\n",
       "      <td>3.46</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>10  2</td>\n",
       "      <td></td>\n",
       "      <td>c, c</td>\n",
       "      <td>Frank</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.18</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>7  8</td>\n",
       "      <td></td>\n",
       "      <td>c, c</td>\n",
       "      <td>Frank</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.92</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>6  8</td>\n",
       "      <td></td>\n",
       "      <td>c, c</td>\n",
       "      <td>Frank</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.04</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>5  2</td>\n",
       "      <td></td>\n",
       "      <td>c, c</td>\n",
       "      <td>Frank</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.47</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>8  2</td>\n",
       "      <td></td>\n",
       "      <td>c, c</td>\n",
       "      <td>Student</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.36, 2.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3  8</td>\n",
       "      <td>2</td>\n",
       "      <td>c, c</td>\n",
       "      <td>Joe</td>\n",
       "      <td>270.0</td>\n",
       "      <td>3.67</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.59</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9  2</td>\n",
       "      <td>8</td>\n",
       "      <td>c, c</td>\n",
       "      <td>Joe</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.56</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4  5</td>\n",
       "      <td>2</td>\n",
       "      <td>c, c</td>\n",
       "      <td>Frank</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-8.84</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.63</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1  2</td>\n",
       "      <td>5</td>\n",
       "      <td>c, c</td>\n",
       "      <td>Joe</td>\n",
       "      <td>90.0</td>\n",
       "      <td>2.27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>10  5</td>\n",
       "      <td>2</td>\n",
       "      <td>c, c</td>\n",
       "      <td>Gaussian</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.22</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>7  2</td>\n",
       "      <td>8</td>\n",
       "      <td>c, c</td>\n",
       "      <td>Joe</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.46</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>6  2</td>\n",
       "      <td>8</td>\n",
       "      <td>c, c</td>\n",
       "      <td>Gaussian</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.43</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>5  8</td>\n",
       "      <td>2</td>\n",
       "      <td>c, c</td>\n",
       "      <td>Clayton</td>\n",
       "      <td>270.0</td>\n",
       "      <td>3.46</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.63</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3  5</td>\n",
       "      <td>8  2</td>\n",
       "      <td>c, c</td>\n",
       "      <td>Student</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.55, 2.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>9  5</td>\n",
       "      <td>2  8</td>\n",
       "      <td>c, c</td>\n",
       "      <td>Frank</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.43</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4  8</td>\n",
       "      <td>5  2</td>\n",
       "      <td>c, c</td>\n",
       "      <td>Clayton</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1  8</td>\n",
       "      <td>2  5</td>\n",
       "      <td>c, c</td>\n",
       "      <td>Gumbel</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1.72</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>10  8</td>\n",
       "      <td>5  2</td>\n",
       "      <td>c, c</td>\n",
       "      <td>Frank</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.92</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>7  5</td>\n",
       "      <td>2  8</td>\n",
       "      <td>c, c</td>\n",
       "      <td>Gumbel</td>\n",
       "      <td>180.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.67</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>6  5</td>\n",
       "      <td>2  8</td>\n",
       "      <td>c, c</td>\n",
       "      <td>Clayton</td>\n",
       "      <td>270.0</td>\n",
       "      <td>1.97</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3  7</td>\n",
       "      <td>5  8  2</td>\n",
       "      <td>c, c</td>\n",
       "      <td>Independence</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>9  6</td>\n",
       "      <td>5  2  8</td>\n",
       "      <td>c, c</td>\n",
       "      <td>Independence</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4  10</td>\n",
       "      <td>8  5  2</td>\n",
       "      <td>c, c</td>\n",
       "      <td>Independence</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1  7</td>\n",
       "      <td>8  2  5</td>\n",
       "      <td>c, c</td>\n",
       "      <td>Independence</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>10  6</td>\n",
       "      <td>8  5  2</td>\n",
       "      <td>c, c</td>\n",
       "      <td>Independence</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>7  6</td>\n",
       "      <td>5  2  8</td>\n",
       "      <td>c, c</td>\n",
       "      <td>Independence</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3  6</td>\n",
       "      <td>7  5  8  2</td>\n",
       "      <td>c, c</td>\n",
       "      <td>Independence</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>9  7</td>\n",
       "      <td>6  5  2  8</td>\n",
       "      <td>c, c</td>\n",
       "      <td>Independence</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4  6</td>\n",
       "      <td>10  8  5  2</td>\n",
       "      <td>c, c</td>\n",
       "      <td>Independence</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1  6</td>\n",
       "      <td>7  8  2  5</td>\n",
       "      <td>c, c</td>\n",
       "      <td>Independence</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>10  7</td>\n",
       "      <td>6  8  5  2</td>\n",
       "      <td>c, c</td>\n",
       "      <td>Independence</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3  10</td>\n",
       "      <td>6  7  5  8  2</td>\n",
       "      <td>c, c</td>\n",
       "      <td>Independence</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>9  1</td>\n",
       "      <td>7  6  5  2  8</td>\n",
       "      <td>c, c</td>\n",
       "      <td>Independence</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>4  7</td>\n",
       "      <td>6  10  8  5  2</td>\n",
       "      <td>c, c</td>\n",
       "      <td>Independence</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1  10</td>\n",
       "      <td>6  7  8  2  5</td>\n",
       "      <td>c, c</td>\n",
       "      <td>Independence</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>3  1</td>\n",
       "      <td>10  6  7  5  8  2</td>\n",
       "      <td>c, c</td>\n",
       "      <td>Independence</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>9  10</td>\n",
       "      <td>1  7  6  5  2  8</td>\n",
       "      <td>c, c</td>\n",
       "      <td>Independence</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>4  1</td>\n",
       "      <td>7  6  10  8  5  2</td>\n",
       "      <td>c, c</td>\n",
       "      <td>Independence</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3  4</td>\n",
       "      <td>1  10  6  7  5  8  2</td>\n",
       "      <td>c, c</td>\n",
       "      <td>Independence</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>9  4</td>\n",
       "      <td>10  1  7  6  5  2  8</td>\n",
       "      <td>c, c</td>\n",
       "      <td>Independence</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3  9</td>\n",
       "      <td>4  1  10  6  7  5  8  2</td>\n",
       "      <td>c, c</td>\n",
       "      <td>Independence</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    tree  edge conditioned variables      conditioning variables  \\\n",
       "0      1     1                  3  2                               \n",
       "1      1     2                  9  8                               \n",
       "2      1     3                  4  2                               \n",
       "3      1     4                  1  5                               \n",
       "4      1     5                 10  2                               \n",
       "5      1     6                  7  8                               \n",
       "6      1     7                  6  8                               \n",
       "7      1     8                  5  2                               \n",
       "8      1     9                  8  2                               \n",
       "9      2     1                  3  8                        2      \n",
       "10     2     2                  9  2                        8      \n",
       "11     2     3                  4  5                        2      \n",
       "12     2     4                  1  2                        5      \n",
       "13     2     5                 10  5                        2      \n",
       "14     2     6                  7  2                        8      \n",
       "15     2     7                  6  2                        8      \n",
       "16     2     8                  5  8                        2      \n",
       "17     3     1                  3  5                     8  2      \n",
       "18     3     2                  9  5                     2  8      \n",
       "19     3     3                  4  8                     5  2      \n",
       "20     3     4                  1  8                     2  5      \n",
       "21     3     5                 10  8                     5  2      \n",
       "22     3     6                  7  5                     2  8      \n",
       "23     3     7                  6  5                     2  8      \n",
       "24     4     1                  3  7                  5  8  2      \n",
       "25     4     2                  9  6                  5  2  8      \n",
       "26     4     3                 4  10                  8  5  2      \n",
       "27     4     4                  1  7                  8  2  5      \n",
       "28     4     5                 10  6                  8  5  2      \n",
       "29     4     6                  7  6                  5  2  8      \n",
       "30     5     1                  3  6               7  5  8  2      \n",
       "31     5     2                  9  7               6  5  2  8      \n",
       "32     5     3                  4  6              10  8  5  2      \n",
       "33     5     4                  1  6               7  8  2  5      \n",
       "34     5     5                 10  7               6  8  5  2      \n",
       "35     6     1                 3  10            6  7  5  8  2      \n",
       "36     6     2                  9  1            7  6  5  2  8      \n",
       "37     6     3                  4  7           6  10  8  5  2      \n",
       "38     6     4                 1  10            6  7  8  2  5      \n",
       "39     7     1                  3  1        10  6  7  5  8  2      \n",
       "40     7     2                 9  10         1  7  6  5  2  8      \n",
       "41     7     3                  4  1        7  6  10  8  5  2      \n",
       "42     8     1                  3  4     1  10  6  7  5  8  2      \n",
       "43     8     2                  9  4     10  1  7  6  5  2  8      \n",
       "44     9     1                  3  9  4  1  10  6  7  5  8  2      \n",
       "\n",
       "        var_types        family  rotation   parameters   df   tau  \\\n",
       "0   c, c                    Joe     270.0         5.44  1.0 -0.70   \n",
       "1       c, c            Student       0.0   0.42, 2.00  2.0  0.28   \n",
       "2        c, c          Gaussian       0.0        -0.34  1.0 -0.22   \n",
       "3   c, c                    Joe     180.0         3.46  1.0  0.57   \n",
       "4     c, c                Frank       0.0        -5.18  1.0 -0.47   \n",
       "5     c, c                Frank       0.0        -3.92  1.0 -0.38   \n",
       "6     c, c                Frank       0.0        -3.04  1.0 -0.31   \n",
       "7     c, c                Frank       0.0         5.17  1.0  0.47   \n",
       "8       c, c            Student       0.0  -0.36, 2.00  2.0 -0.23   \n",
       "9   c, c                    Joe     270.0         3.67  1.0 -0.59   \n",
       "10  c, c                    Joe       0.0         3.56  1.0  0.58   \n",
       "11    c, c                Frank       0.0        -8.84  1.0 -0.63   \n",
       "12  c, c                    Joe      90.0         2.27  1.0 -0.41   \n",
       "13       c, c          Gaussian       0.0         0.34  1.0  0.22   \n",
       "14  c, c                    Joe       0.0         2.60  1.0  0.46   \n",
       "15       c, c          Gaussian       0.0        -0.43  1.0 -0.28   \n",
       "16      c, c            Clayton     270.0         3.46  1.0 -0.63   \n",
       "17      c, c            Student       0.0  -0.55, 2.00  2.0 -0.37   \n",
       "18    c, c                Frank       0.0        -4.43  1.0 -0.42   \n",
       "19      c, c            Clayton      90.0         0.98  1.0 -0.33   \n",
       "20     c, c              Gumbel      90.0         1.72  1.0 -0.42   \n",
       "21    c, c                Frank       0.0         5.92  1.0  0.51   \n",
       "22     c, c              Gumbel     180.0         3.00  1.0  0.67   \n",
       "23      c, c            Clayton     270.0         1.97  1.0 -0.50   \n",
       "24           c, c  Independence       NaN               0.0   NaN   \n",
       "25           c, c  Independence       NaN               0.0   NaN   \n",
       "26           c, c  Independence       NaN               0.0   NaN   \n",
       "27           c, c  Independence       NaN               0.0   NaN   \n",
       "28           c, c  Independence       NaN               0.0   NaN   \n",
       "29           c, c  Independence       NaN               0.0   NaN   \n",
       "30           c, c  Independence       NaN               0.0   NaN   \n",
       "31           c, c  Independence       NaN               0.0   NaN   \n",
       "32           c, c  Independence       NaN               0.0   NaN   \n",
       "33           c, c  Independence       NaN               0.0   NaN   \n",
       "34           c, c  Independence       NaN               0.0   NaN   \n",
       "35           c, c  Independence       NaN               0.0   NaN   \n",
       "36           c, c  Independence       NaN               0.0   NaN   \n",
       "37           c, c  Independence       NaN               0.0   NaN   \n",
       "38           c, c  Independence       NaN               0.0   NaN   \n",
       "39           c, c  Independence       NaN               0.0   NaN   \n",
       "40           c, c  Independence       NaN               0.0   NaN   \n",
       "41           c, c  Independence       NaN               0.0   NaN   \n",
       "42           c, c  Independence       NaN               0.0   NaN   \n",
       "43           c, c  Independence       NaN               0.0   NaN   \n",
       "44           c, c  Independence       NaN               NaN  0.00   \n",
       "\n",
       "    conditioned variable 1  conditioned variable 2  var_row  var_col  \\\n",
       "0                        3                       2        3        2   \n",
       "1                        9                       8        9        8   \n",
       "2                        4                       2        4        2   \n",
       "3                        1                       5        5        1   \n",
       "4                       10                       2       10        2   \n",
       "5                        7                       8        8        7   \n",
       "6                        6                       8        8        6   \n",
       "7                        5                       2        5        2   \n",
       "8                        8                       2        8        2   \n",
       "9                        3                       8        8        3   \n",
       "10                       9                       2        9        2   \n",
       "11                       4                       5        5        4   \n",
       "12                       1                       2        2        1   \n",
       "13                      10                       5       10        5   \n",
       "14                       7                       2        7        2   \n",
       "15                       6                       2        6        2   \n",
       "16                       5                       8        8        5   \n",
       "17                       3                       5        5        3   \n",
       "18                       9                       5        9        5   \n",
       "19                       4                       8        8        4   \n",
       "20                       1                       8        8        1   \n",
       "21                      10                       8       10        8   \n",
       "22                       7                       5        7        5   \n",
       "23                       6                       5        6        5   \n",
       "24                       3                       7        7        3   \n",
       "25                       9                       6        9        6   \n",
       "26                       4                      10       10        4   \n",
       "27                       1                       7        7        1   \n",
       "28                      10                       6       10        6   \n",
       "29                       7                       6        7        6   \n",
       "30                       3                       6        6        3   \n",
       "31                       9                       7        9        7   \n",
       "32                       4                       6        6        4   \n",
       "33                       1                       6        6        1   \n",
       "34                      10                       7       10        7   \n",
       "35                       3                      10       10        3   \n",
       "36                       9                       1        9        1   \n",
       "37                       4                       7        7        4   \n",
       "38                       1                      10       10        1   \n",
       "39                       3                       1        3        1   \n",
       "40                       9                      10       10        9   \n",
       "41                       4                       1        4        1   \n",
       "42                       3                       4        4        3   \n",
       "43                       9                       4        9        4   \n",
       "44                       3                       9        9        3   \n",
       "\n",
       "    dependence  \n",
       "0            1  \n",
       "1            1  \n",
       "2            1  \n",
       "3            1  \n",
       "4            1  \n",
       "5            1  \n",
       "6            1  \n",
       "7            1  \n",
       "8            1  \n",
       "9            1  \n",
       "10           1  \n",
       "11           1  \n",
       "12           1  \n",
       "13           1  \n",
       "14           1  \n",
       "15           1  \n",
       "16           1  \n",
       "17           0  \n",
       "18           0  \n",
       "19           0  \n",
       "20           0  \n",
       "21           0  \n",
       "22           0  \n",
       "23           0  \n",
       "24           0  \n",
       "25           0  \n",
       "26           0  \n",
       "27           0  \n",
       "28           0  \n",
       "29           0  \n",
       "30           0  \n",
       "31           0  \n",
       "32           0  \n",
       "33           0  \n",
       "34           0  \n",
       "35           0  \n",
       "36           0  \n",
       "37           0  \n",
       "38           0  \n",
       "39           0  \n",
       "40           0  \n",
       "41           0  \n",
       "42           0  \n",
       "43           0  \n",
       "44           0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = compute_conditional_dependence_table(vine_model)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "639cb6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/57/_f_fv4s97k300zslnyj86dxc0000gn/T/ipykernel_11787/3150062370.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_true_structure_sub[\"var_row\"] = df_true_structure_sub[\"var_row\"] - 1\n",
      "/var/folders/57/_f_fv4s97k300zslnyj86dxc0000gn/T/ipykernel_11787/3150062370.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_true_structure_sub[\"var_col\"] = df_true_structure_sub[\"var_col\"] - 1\n"
     ]
    }
   ],
   "source": [
    "#creating a table to copare the true dependence structure later on to what the gtm learned\n",
    "df_true_structure = df[[\"tree\",\"edge\",\"conditioned variables\", \"conditioned variable 1\", \"conditioned variable 2\", \"dependence\", \"var_row\", \"var_col\"]]\n",
    "df_true_structure_sub = df_true_structure[[\"var_row\", \"var_col\", \"dependence\"]]\n",
    "df_true_structure_sub[\"var_row\"] = df_true_structure_sub[\"var_row\"] - 1\n",
    "df_true_structure_sub[\"var_col\"] = df_true_structure_sub[\"var_col\"] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13083e1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vine_model.plot(tree=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91426a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "N_train = 2000\n",
    "simulated_data_uniform_train = vine_model.simulate(n=N_train)\n",
    "simulated_data_train = torch.distributions.Normal(0,1).icdf(torch.tensor(simulated_data_uniform_train)).float()\n",
    "\n",
    "# Validate\n",
    "N_validate = 2000\n",
    "simulated_data_uniform_validate = vine_model.simulate(n=N_validate)\n",
    "simulated_data_validate = torch.distributions.Normal(0,1).icdf(torch.tensor(simulated_data_uniform_validate)).float()\n",
    "\n",
    "# Test\n",
    "N_test = 20000\n",
    "simulated_data_uniform_test = vine_model.simulate(n=N_test)\n",
    "simulated_data_test = torch.distributions.Normal(0,1).icdf(torch.tensor(simulated_data_uniform_test)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89a347f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/57/_f_fv4s97k300zslnyj86dxc0000gn/T/ipykernel_11787/1520218578.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  log_marginals = torch.distributions.Normal(0,1).log_prob(torch.tensor(simulated_data_train)).sum(1)\n",
      "/var/folders/57/_f_fv4s97k300zslnyj86dxc0000gn/T/ipykernel_11787/1520218578.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  log_marginals = torch.distributions.Normal(0,1).log_prob(torch.tensor(simulated_data_validate)).sum(1)\n",
      "/var/folders/57/_f_fv4s97k300zslnyj86dxc0000gn/T/ipykernel_11787/1520218578.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  log_marginals = torch.distributions.Normal(0,1).log_prob(torch.tensor(simulated_data_test)).sum(1)\n"
     ]
    }
   ],
   "source": [
    "loglik_copula = np.log(vine_model.pdf(simulated_data_uniform_train))\n",
    "log_marginals = torch.distributions.Normal(0,1).log_prob(torch.tensor(simulated_data_train)).sum(1)\n",
    "loglik_true_train = torch.tensor(loglik_copula) + log_marginals\n",
    "\n",
    "loglik_copula = np.log(vine_model.pdf(simulated_data_uniform_validate))\n",
    "log_marginals = torch.distributions.Normal(0,1).log_prob(torch.tensor(simulated_data_validate)).sum(1)\n",
    "loglik_true_validate = torch.tensor(loglik_copula) + log_marginals\n",
    "\n",
    "loglik_copula = np.log(vine_model.pdf(simulated_data_uniform_test))\n",
    "log_marginals = torch.distributions.Normal(0,1).log_prob(torch.tensor(simulated_data_test)).sum(1)\n",
    "loglik_true_test = torch.tensor(loglik_copula) + log_marginals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ab3f371",
   "metadata": {},
   "outputs": [],
   "source": [
    "copula_pv_est = vine_model\n",
    "copula_pv_est.fit(simulated_data_uniform_train)\n",
    "means = simulated_data_train.mean(0)\n",
    "vars = simulated_data_train.var(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "357cb0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/57/_f_fv4s97k300zslnyj86dxc0000gn/T/ipykernel_11787/2452348847.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  log_marginals = torch.distributions.Normal(means,vars).log_prob(torch.tensor(simulated_data_train)).sum(1)\n",
      "/var/folders/57/_f_fv4s97k300zslnyj86dxc0000gn/T/ipykernel_11787/2452348847.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  log_marginals = torch.distributions.Normal(means,vars).log_prob(torch.tensor(simulated_data_validate)).sum(1)\n",
      "/var/folders/57/_f_fv4s97k300zslnyj86dxc0000gn/T/ipykernel_11787/2452348847.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  log_marginals = torch.distributions.Normal(means,vars).log_prob(torch.tensor(simulated_data_test)).sum(1)\n"
     ]
    }
   ],
   "source": [
    "loglik_copula = np.log(copula_pv_est.pdf(simulated_data_uniform_train))\n",
    "log_marginals = torch.distributions.Normal(means,vars).log_prob(torch.tensor(simulated_data_train)).sum(1)\n",
    "loglik_true_est_train = torch.tensor(loglik_copula) + log_marginals\n",
    "\n",
    "loglik_copula = np.log(copula_pv_est.pdf(simulated_data_uniform_validate))\n",
    "log_marginals = torch.distributions.Normal(means,vars).log_prob(torch.tensor(simulated_data_validate)).sum(1)\n",
    "loglik_true_est_validate = torch.tensor(loglik_copula) + log_marginals\n",
    "\n",
    "loglik_copula = np.log(copula_pv_est.pdf(simulated_data_uniform_test))\n",
    "log_marginals = torch.distributions.Normal(means,vars).log_prob(torch.tensor(simulated_data_test)).sum(1)\n",
    "loglik_true_est_test = torch.tensor(loglik_copula) + log_marginals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2036680f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#_ = plot_densities(simulated_data_train, x_lim=[-4,4], y_lim=[-4,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "133db9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#_ = plot_marginals(simulated_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83659cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and DataLoader\n",
    "dataset_train = Generic_Dataset(simulated_data_train)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=N_train)\n",
    "\n",
    "dataset_validate = Generic_Dataset(simulated_data_validate)\n",
    "dataloader_validate = DataLoader(dataset_validate, batch_size=N_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d69048b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2000, 10])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here you can see that we load the full data, so not batches\n",
    "# Model is just implemented with dataloaders because that is eeded for huge datasets in bioinformatics\n",
    "data_iter = iter(dataloader_train)\n",
    "sample = next(data_iter)\n",
    "sample.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c111eb",
   "metadata": {},
   "source": [
    "### 2. Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef92cf12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthiasherp/Desktop/phd_github_repositories/gtm/gtm/gtm_layers/transformation_layer.py:58: UserWarning: Warning: Varying Spline Degree for each Dimension is not implemented for Bernstein, only for B-Spline.\n",
      "  warnings.warn(\"Warning: Varying Spline Degree for each Dimension is not implemented for Bernstein, only for B-Spline.\")\n",
      "/Users/matthiasherp/Desktop/phd_github_repositories/gtm/gtm/gtm_layers/transformation_layer.py:63: UserWarning: Bernstein polynomial penalization is not implemented yet. only returns zeros hardcoded in bernstein_prediction.py fct\n",
      "  warnings.warn(\"Bernstein polynomial penalization is not implemented yet. only returns zeros hardcoded in bernstein_prediction.py fct\")\n",
      "/Users/matthiasherp/Desktop/phd_github_repositories/gtm/gtm/gtm_splines/bernstein_prediction_vectorized.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.lgamma(torch.tensor(n - i + 1.0, device=device))\n",
      "/Users/matthiasherp/Desktop/phd_github_repositories/gtm/gtm/gtm_layers/decorrelation_layer.py:25: UserWarning: Bernstein polynomial penalization is not implemented yet. only returns zeros hardcoded in bernstein_prediction.py fct\n",
      "  warnings.warn(\"Bernstein polynomial penalization is not implemented yet. only returns zeros hardcoded in bernstein_prediction.py fct\")\n"
     ]
    }
   ],
   "source": [
    "model = GTM(\n",
    "    transformation_spline_range=list([[-10], [10]]), \n",
    "    decorrelation_spline_range=list([[-10], [10]]), \n",
    "    degree_decorrelation=5,\n",
    "    degree_transformations=10,\n",
    "    num_decorr_layers=4,\n",
    "    num_trans_layers=1,\n",
    "    number_variables=10,\n",
    "    calc_method_bspline=\"deBoor\",\n",
    "    spline_decorrelation=\"bernstein\", #\"bspline\", #\"bernstein\",\n",
    "    spline_transformation=\"bernstein\",\n",
    "    affine_decorr_layer=False,\n",
    "    span_restriction=\"reluler\",\n",
    "    device=\"cpu\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d41ca3e",
   "metadata": {},
   "source": [
    "### 3. Hyperparameter Tune and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "807c8eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthiasherp/anaconda3/envs/mctm_pytorch/lib/python3.11/site-packages/optuna/samplers/_tpe/sampler.py:319: ExperimentalWarning: ``multivariate`` option is an experimental feature. The interface can change in the future.\n",
      "  warnings.warn(\n",
      "[I 2025-05-23 14:40:52,924] A new study created in RDB with name: no-name-dffe3476-fe2b-43fa-95c5-287ff99c57da\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Trial has the Hyperparameters: penvalueridge_opt: 0   penfirstridge_opt: 16.906268195445964   pensecondridge_opt: 10.304419198248382   ctm_pensecondridge_opt: 0   lambda_penalty_params_opt: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 202/1000 [00:32<02:07,  6.28it/s]\n",
      "[I 2025-05-23 14:41:25,204] Trial 0 finished with value: -7.169945240020752 and parameters: {'penfirstridge': 16.906268195445964, 'pensecondridge': 10.304419198248382}. Best is trial 0 with value: -7.169945240020752.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Trial has the Hyperparameters: penvalueridge_opt: 0   penfirstridge_opt: 13.399828001950633   pensecondridge_opt: 0.8235903833827221   ctm_pensecondridge_opt: 0   lambda_penalty_params_opt: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 306/1000 [00:46<01:44,  6.64it/s]\n",
      "[I 2025-05-23 14:42:11,404] Trial 1 finished with value: -6.975780010223389 and parameters: {'penfirstridge': 13.399828001950633, 'pensecondridge': 0.8235903833827221}. Best is trial 1 with value: -6.975780010223389.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Trial has the Hyperparameters: penvalueridge_opt: 0   penfirstridge_opt: 29.28271661603939   pensecondridge_opt: 6.991828377837199   ctm_pensecondridge_opt: 0   lambda_penalty_params_opt: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 27/1000 [00:04<02:40,  6.05it/s]\n",
      "[I 2025-05-23 14:42:15,946] Trial 2 finished with value: -8.592982292175293 and parameters: {'penfirstridge': 29.28271661603939, 'pensecondridge': 6.991828377837199}. Best is trial 1 with value: -6.975780010223389.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Trial has the Hyperparameters: penvalueridge_opt: 0   penfirstridge_opt: 8.467810536227393   pensecondridge_opt: 14.701210033843113   ctm_pensecondridge_opt: 0   lambda_penalty_params_opt: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 254/1000 [00:36<01:48,  6.89it/s]\n",
      "[I 2025-05-23 14:42:52,917] Trial 3 finished with value: -6.964595794677734 and parameters: {'penfirstridge': 8.467810536227393, 'pensecondridge': 14.701210033843113}. Best is trial 3 with value: -6.964595794677734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Trial has the Hyperparameters: penvalueridge_opt: 0   penfirstridge_opt: 26.559433619628983   pensecondridge_opt: 23.229511846182668   ctm_pensecondridge_opt: 0   lambda_penalty_params_opt: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 155/1000 [00:21<01:58,  7.14it/s]\n",
      "[I 2025-05-23 14:43:14,708] Trial 4 finished with value: -7.3560566902160645 and parameters: {'penfirstridge': 26.559433619628983, 'pensecondridge': 23.229511846182668}. Best is trial 3 with value: -6.964595794677734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Trial has the Hyperparameters: penvalueridge_opt: 0   penfirstridge_opt: 27.572481976786186   pensecondridge_opt: 3.351197308102136   ctm_pensecondridge_opt: 0   lambda_penalty_params_opt: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 115/1000 [00:16<02:10,  6.79it/s]\n",
      "[I 2025-05-23 14:43:31,714] Trial 5 finished with value: -7.708654880523682 and parameters: {'penfirstridge': 27.572481976786186, 'pensecondridge': 3.351197308102136}. Best is trial 3 with value: -6.964595794677734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Trial has the Hyperparameters: penvalueridge_opt: 0   penfirstridge_opt: 10.887684267540102   pensecondridge_opt: 5.815606251320858   ctm_pensecondridge_opt: 0   lambda_penalty_params_opt: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 55/1000 [00:07<02:14,  7.02it/s]\n",
      "[I 2025-05-23 14:43:39,635] Trial 6 finished with value: -7.962723255157471 and parameters: {'penfirstridge': 10.887684267540102, 'pensecondridge': 5.815606251320858}. Best is trial 3 with value: -6.964595794677734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Trial has the Hyperparameters: penvalueridge_opt: 0   penfirstridge_opt: 24.1483827666606   pensecondridge_opt: 20.78382324708186   ctm_pensecondridge_opt: 0   lambda_penalty_params_opt: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 193/1000 [00:27<01:55,  6.96it/s]\n",
      "[I 2025-05-23 14:44:07,454] Trial 7 finished with value: -7.280590057373047 and parameters: {'penfirstridge': 24.1483827666606, 'pensecondridge': 20.78382324708186}. Best is trial 3 with value: -6.964595794677734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Trial has the Hyperparameters: penvalueridge_opt: 0   penfirstridge_opt: 11.266841862788063   pensecondridge_opt: 4.225987557219937   ctm_pensecondridge_opt: 0   lambda_penalty_params_opt: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 137/1000 [00:20<02:07,  6.76it/s]\n",
      "[I 2025-05-23 14:44:27,812] Trial 8 finished with value: -7.190523624420166 and parameters: {'penfirstridge': 11.266841862788063, 'pensecondridge': 4.225987557219937}. Best is trial 3 with value: -6.964595794677734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Trial has the Hyperparameters: penvalueridge_opt: 0   penfirstridge_opt: 29.685195523380198   pensecondridge_opt: 24.869794883999322   ctm_pensecondridge_opt: 0   lambda_penalty_params_opt: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 79/1000 [00:11<02:13,  6.88it/s]\n",
      "[I 2025-05-23 14:44:39,382] Trial 9 finished with value: -8.03335952758789 and parameters: {'penfirstridge': 29.685195523380198, 'pensecondridge': 24.869794883999322}. Best is trial 3 with value: -6.964595794677734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Trial has the Hyperparameters: penvalueridge_opt: 0   penfirstridge_opt: 4.6984171537176955   pensecondridge_opt: 29.84479001342971   ctm_pensecondridge_opt: 0   lambda_penalty_params_opt: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 118/1000 [00:16<02:03,  7.16it/s]\n",
      "[I 2025-05-23 14:44:55,939] Trial 10 finished with value: -7.404181480407715 and parameters: {'penfirstridge': 4.6984171537176955, 'pensecondridge': 29.84479001342971}. Best is trial 3 with value: -6.964595794677734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Trial has the Hyperparameters: penvalueridge_opt: 0   penfirstridge_opt: 7.205473812885041   pensecondridge_opt: 19.40826156856253   ctm_pensecondridge_opt: 0   lambda_penalty_params_opt: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 183/1000 [00:25<01:52,  7.23it/s]\n",
      "[I 2025-05-23 14:45:21,329] Trial 11 finished with value: -7.140079975128174 and parameters: {'penfirstridge': 7.205473812885041, 'pensecondridge': 19.40826156856253}. Best is trial 3 with value: -6.964595794677734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Trial has the Hyperparameters: penvalueridge_opt: 0   penfirstridge_opt: 2.9568730423136405   pensecondridge_opt: 3.324152142007751   ctm_pensecondridge_opt: 0   lambda_penalty_params_opt: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 169/1000 [00:23<01:56,  7.15it/s]\n",
      "[I 2025-05-23 14:45:45,061] Trial 12 finished with value: -6.959198951721191 and parameters: {'penfirstridge': 2.9568730423136405, 'pensecondridge': 3.324152142007751}. Best is trial 12 with value: -6.959198951721191.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Trial has the Hyperparameters: penvalueridge_opt: 0   penfirstridge_opt: 0.38354107548009786   pensecondridge_opt: 7.404244079609736   ctm_pensecondridge_opt: 0   lambda_penalty_params_opt: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 218/1000 [00:31<01:51,  7.02it/s]\n",
      "[I 2025-05-23 14:46:16,196] Trial 13 finished with value: -6.84974479675293 and parameters: {'penfirstridge': 0.38354107548009786, 'pensecondridge': 7.404244079609736}. Best is trial 13 with value: -6.84974479675293.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Trial has the Hyperparameters: penvalueridge_opt: 0   penfirstridge_opt: 2.468947501500691   pensecondridge_opt: 5.7021863766492835   ctm_pensecondridge_opt: 0   lambda_penalty_params_opt: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 173/1000 [00:24<01:56,  7.10it/s]\n",
      "[I 2025-05-23 14:46:40,641] Trial 14 finished with value: -6.9815239906311035 and parameters: {'penfirstridge': 2.468947501500691, 'pensecondridge': 5.7021863766492835}. Best is trial 13 with value: -6.84974479675293.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Trial has the Hyperparameters: penvalueridge_opt: 0   penfirstridge_opt: 0.2759254392434347   pensecondridge_opt: 16.971261449481013   ctm_pensecondridge_opt: 0   lambda_penalty_params_opt: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 189/1000 [00:26<01:52,  7.20it/s]\n",
      "[I 2025-05-23 14:47:06,983] Trial 15 finished with value: -7.079928874969482 and parameters: {'penfirstridge': 0.2759254392434347, 'pensecondridge': 16.971261449481013}. Best is trial 13 with value: -6.84974479675293.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Trial has the Hyperparameters: penvalueridge_opt: 0   penfirstridge_opt: 5.025490545884263   pensecondridge_opt: 0.7721753066744625   ctm_pensecondridge_opt: 0   lambda_penalty_params_opt: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 131/1000 [00:19<02:07,  6.79it/s]\n",
      "[I 2025-05-23 14:47:26,355] Trial 16 finished with value: -7.05792760848999 and parameters: {'penfirstridge': 5.025490545884263, 'pensecondridge': 0.7721753066744625}. Best is trial 13 with value: -6.84974479675293.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Trial has the Hyperparameters: penvalueridge_opt: 0   penfirstridge_opt: 0.3981563266873863   pensecondridge_opt: 9.36999008667733   ctm_pensecondridge_opt: 0   lambda_penalty_params_opt: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 229/1000 [00:32<01:48,  7.13it/s]\n",
      "[I 2025-05-23 14:47:58,566] Trial 17 finished with value: -6.859644412994385 and parameters: {'penfirstridge': 0.3981563266873863, 'pensecondridge': 9.36999008667733}. Best is trial 13 with value: -6.84974479675293.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Trial has the Hyperparameters: penvalueridge_opt: 0   penfirstridge_opt: 2.109382522723287   pensecondridge_opt: 12.35839047693668   ctm_pensecondridge_opt: 0   lambda_penalty_params_opt: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 180/1000 [00:25<01:54,  7.14it/s]\n",
      "[I 2025-05-23 14:48:23,863] Trial 18 finished with value: -7.052333831787109 and parameters: {'penfirstridge': 2.109382522723287, 'pensecondridge': 12.35839047693668}. Best is trial 13 with value: -6.84974479675293.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Trial has the Hyperparameters: penvalueridge_opt: 0   penfirstridge_opt: 12.838999323253626   pensecondridge_opt: 24.80853876130514   ctm_pensecondridge_opt: 0   lambda_penalty_params_opt: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 84/1000 [00:11<02:09,  7.08it/s]\n",
      "[I 2025-05-23 14:48:35,814] Trial 19 finished with value: -7.795208930969238 and parameters: {'penfirstridge': 12.838999323253626, 'pensecondridge': 24.80853876130514}. Best is trial 13 with value: -6.84974479675293.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyperparameter_tuning done\n"
     ]
    }
   ],
   "source": [
    "study = model.hyperparameter_tune_penalties( \n",
    "                                train_dataloader=dataloader_train, \n",
    "                                validate_dataloader=dataloader_validate, \n",
    "                                penvalueridge = [0], #[\"sample\"],\n",
    "                                penfirstridge = [\"sample\"],\n",
    "                                pensecondridge = [\"sample\"],\n",
    "                                ctm_pensecondridge = [0], #[\"sample\"],\n",
    "                                lambda_penalty_params = [0], #[\"sample\"], #[0],\n",
    "                                train_covariates=False, \n",
    "                                validate_covariates=False, \n",
    "                                adaptive_lasso_weights_matrix = False,\n",
    "                                learning_rate=1, \n",
    "                                iterations=1000, \n",
    "                                patience=5, \n",
    "                                min_delta=1e-7, \n",
    "                                optimizer='LBFGS', \n",
    "                                lambda_penalty_mode=\"square\", \n",
    "                                objective_type=\"negloglik\", \n",
    "                                seperate_copula_training=False,\n",
    "                                max_batches_per_iter=False,\n",
    "                                tuning_mode=\"optuna\",\n",
    "                                cross_validation_folds=False,\n",
    "                                random_state_KFold=42,\n",
    "                                device=None,\n",
    "                                pretrained_transformation_layer=False,\n",
    "                                n_trials=20,\n",
    "                                temp_folder=\".\", \n",
    "                                study_name=None)\n",
    "    \n",
    "penalty_params=torch.FloatTensor([\n",
    "                            0, #study.best_params[\"penvalueridge\"],\n",
    "                            study.best_params[\"penfirstridge\"],\n",
    "                            study.best_params[\"pensecondridge\"],\n",
    "                            0 #study.best_params[\"ctm_pensecondridge\"]\n",
    "                              ])\n",
    "adaptive_lasso_weights_matrix = False\n",
    "lambda_penalty_params= False #study.best_params[\"lambda_penalty_params\"] #False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "17091610",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GTM(\n",
    "    transformation_spline_range=list([[-10], [10]]), \n",
    "    decorrelation_spline_range=list([[-10], [10]]), \n",
    "    degree_decorrelation=20,\n",
    "    degree_transformations=15,\n",
    "    num_decorr_layers=4,\n",
    "    num_trans_layers=1,\n",
    "    number_variables=10,\n",
    "    calc_method_bspline=\"deBoor\",\n",
    "    spline_decorrelation=\"bspline\", #\"bspline\", #\"bernstein\",\n",
    "    affine_decorr_layer=False,\n",
    "    span_restriction=\"reluler\",\n",
    "    device=\"cpu\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fa875cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty_params=torch.FloatTensor([\n",
    "                            0, #study.best_params[\"penvalueridge\"],\n",
    "                            0,\n",
    "                            0,\n",
    "                            0 #study.best_params[\"ctm_pensecondridge\"]\n",
    "                              ])\n",
    "adaptive_lasso_weights_matrix = False\n",
    "lambda_penalty_params= False #study.best_params[\"lambda_penalty_params\"] #False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f19f1a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthiasherp/Desktop/phd_github_repositories/gtm/gtm/gtm_model/gtm.py:296: UserWarning: Optimiser for pretrain_tranformation_layer is always LBFGS. If this is an issue change the code.\n",
      "  warnings.warn(\"Optimiser for pretrain_tranformation_layer is always LBFGS. If this is an issue change the code.\")\n",
      "  2%|▏         | 24/1000 [00:00<00:30, 31.68it/s]\n"
     ]
    }
   ],
   "source": [
    "# pretrain the marginal transformations\n",
    "_ = model.pretrain_tranformation_layer(dataloader_train, iterations=1000, max_batches_per_iter=False, penalty_params=penalty_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b7429a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 164/1000 [00:25<02:11,  6.36it/s]\n"
     ]
    }
   ],
   "source": [
    "# train the joint model\n",
    "_ = model.__train__(train_dataloader=dataloader_train, validate_dataloader=dataloader_validate, iterations=1000, optimizer=\"LBFGS\", learning_rate=1,\n",
    "                penalty_params=penalty_params, adaptive_lasso_weights_matrix=adaptive_lasso_weights_matrix, lambda_penalty_params=lambda_penalty_params, \n",
    "                max_batches_per_iter=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5eebff",
   "metadata": {},
   "source": [
    "### 4. Compare Fit to Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b910002",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_likelihood_train_gtm = model.log_likelihood(simulated_data_train)\n",
    "log_likelihood_validate_gtm = model.log_likelihood(simulated_data_validate)\n",
    "log_likelihood_test_gtm = model.log_likelihood(simulated_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ad6c5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate the Multivariate Normal Distribution as Model\n",
    "mean_mvn_model = simulated_data_train.mean(0)  # 0 to do mean across dim 0 not globally\n",
    "cov_mvn_model = simulated_data_train.T.cov()\n",
    "mvn_model = torch.distributions.MultivariateNormal(loc=mean_mvn_model, covariance_matrix=cov_mvn_model)\n",
    "log_likelihood_train_gaussian = mvn_model.log_prob(simulated_data_train)\n",
    "log_likelihood_validate_gaussian = mvn_model.log_prob(simulated_data_validate)\n",
    "log_likelihood_test_gaussian = mvn_model.log_prob(simulated_data_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "095880b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KLD GTM      Train Data:  0.8186\n",
      "KLD Gaussian Train Data:  3.5949\n",
      "KLD Copula   Train Data:  0.0342\n",
      "KLD GTM      Test  Data:  1.1349\n",
      "KLD Gaussian Test  Data:  3.6777\n",
      "KLD Copula   Test  Data:  0.0862\n"
     ]
    }
   ],
   "source": [
    "print(\"KLD GTM      Train Data: \",np.round(torch.mean(loglik_true_train - log_likelihood_train_gtm).item(),4) )\n",
    "print(\"KLD Gaussian Train Data: \",np.round(torch.mean(loglik_true_train - log_likelihood_train_gaussian).item(),4) )\n",
    "print(\"KLD Copula   Train Data: \",np.round(torch.mean(loglik_true_train - loglik_true_est_train).item(),4) )\n",
    "\n",
    "print(\"KLD GTM      Test  Data: \",np.round(torch.mean(loglik_true_test - log_likelihood_test_gtm).item(),4) )\n",
    "print(\"KLD Gaussian Test  Data: \",np.round(torch.mean(loglik_true_test - log_likelihood_test_gaussian).item(),4) )\n",
    "print(\"KLD Copula   Test  Data: \",np.round(torch.mean(loglik_true_test - loglik_true_est_test).item(),4) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb85f4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KLD GTM      Train Data:  0.8186\n",
      "KLD Gaussian Train Data:  3.5949\n",
      "KLD Copula   Train Data:  0.0342\n",
      "KLD GTM      Test  Data:  1.1349\n",
      "KLD Gaussian Test  Data:  3.6777\n",
      "KLD Copula   Test  Data:  0.0862\n"
     ]
    }
   ],
   "source": [
    "print(\"KLD GTM      Train Data: \",np.round(torch.mean(loglik_true_train - log_likelihood_train_gtm).item(),4) )\n",
    "print(\"KLD Gaussian Train Data: \",np.round(torch.mean(loglik_true_train - log_likelihood_train_gaussian).item(),4) )\n",
    "print(\"KLD Copula   Train Data: \",np.round(torch.mean(loglik_true_train - loglik_true_est_train).item(),4) )\n",
    "\n",
    "print(\"KLD GTM      Test  Data: \",np.round(torch.mean(loglik_true_test - log_likelihood_test_gtm).item(),4) )\n",
    "print(\"KLD Gaussian Test  Data: \",np.round(torch.mean(loglik_true_test - log_likelihood_test_gaussian).item(),4) )\n",
    "print(\"KLD Copula   Test  Data: \",np.round(torch.mean(loglik_true_test - loglik_true_est_test).item(),4) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0954f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KLD GTM      Train Data:  0.8186\n",
      "KLD Gaussian Train Data:  3.5949\n",
      "KLD Copula   Train Data:  0.0342\n",
      "KLD GTM      Test  Data:  1.1349\n",
      "KLD Gaussian Test  Data:  3.6777\n",
      "KLD Copula   Test  Data:  0.0862\n"
     ]
    }
   ],
   "source": [
    "print(\"KLD GTM      Train Data: \",np.round(torch.mean(loglik_true_train - log_likelihood_train_gtm).item(),4) )\n",
    "print(\"KLD Gaussian Train Data: \",np.round(torch.mean(loglik_true_train - log_likelihood_train_gaussian).item(),4) )\n",
    "print(\"KLD Copula   Train Data: \",np.round(torch.mean(loglik_true_train - loglik_true_est_train).item(),4) )\n",
    "\n",
    "print(\"KLD GTM      Test  Data: \",np.round(torch.mean(loglik_true_test - log_likelihood_test_gtm).item(),4) )\n",
    "print(\"KLD Gaussian Test  Data: \",np.round(torch.mean(loglik_true_test - log_likelihood_test_gaussian).item(),4) )\n",
    "print(\"KLD Copula   Test  Data: \",np.round(torch.mean(loglik_true_test - loglik_true_est_test).item(),4) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a318b6d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mstop\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69391a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KLD GTM      Train Data:  0.9591\n",
      "KLD Gaussian Train Data:  3.4334\n",
      "KLD Copula   Train Data:  0.0186\n",
      "KLD GTM      Test  Data:  1.1836\n",
      "KLD Gaussian Test  Data:  3.4478\n",
      "KLD Copula   Test  Data:  0.0467\n"
     ]
    }
   ],
   "source": [
    "print(\"KLD GTM      Train Data: \",np.round(torch.mean(loglik_true_train - log_likelihood_train_gtm).item(),4) )\n",
    "print(\"KLD Gaussian Train Data: \",np.round(torch.mean(loglik_true_train - log_likelihood_train_gaussian).item(),4) )\n",
    "print(\"KLD Copula   Train Data: \",np.round(torch.mean(loglik_true_train - loglik_true_est_train).item(),4) )\n",
    "\n",
    "print(\"KLD GTM      Test  Data: \",np.round(torch.mean(loglik_true_test - log_likelihood_test_gtm).item(),4) )\n",
    "print(\"KLD Gaussian Test  Data: \",np.round(torch.mean(loglik_true_test - log_likelihood_test_gaussian).item(),4) )\n",
    "print(\"KLD Copula   Test  Data: \",np.round(torch.mean(loglik_true_test - loglik_true_est_test).item(),4) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2f1859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KLD GTM      Train Data:  0.8188\n",
      "KLD Gaussian Train Data:  3.4334\n",
      "KLD Copula   Train Data:  0.0186\n",
      "KLD GTM      Test  Data:  1.082\n",
      "KLD Gaussian Test  Data:  3.4478\n",
      "KLD Copula   Test  Data:  0.0467\n"
     ]
    }
   ],
   "source": [
    "print(\"KLD GTM      Train Data: \",np.round(torch.mean(loglik_true_train - log_likelihood_train_gtm).item(),4) )\n",
    "print(\"KLD Gaussian Train Data: \",np.round(torch.mean(loglik_true_train - log_likelihood_train_gaussian).item(),4) )\n",
    "print(\"KLD Copula   Train Data: \",np.round(torch.mean(loglik_true_train - loglik_true_est_train).item(),4) )\n",
    "\n",
    "print(\"KLD GTM      Test  Data: \",np.round(torch.mean(loglik_true_test - log_likelihood_test_gtm).item(),4) )\n",
    "print(\"KLD Gaussian Test  Data: \",np.round(torch.mean(loglik_true_test - log_likelihood_test_gaussian).item(),4) )\n",
    "print(\"KLD Copula   Test  Data: \",np.round(torch.mean(loglik_true_test - loglik_true_est_test).item(),4) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941037db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "#torch.save(model, \"10D_rvine_model_state_dict.pth\")\n",
    "#model = torch.load(\"./10D_rvine_model_state_dict.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974098c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mstop\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c453d3c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3ee4e5d",
   "metadata": {},
   "source": [
    "### 5. Evaluate and Plot GTM Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0990e4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.approximate_transformation_inverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6637a565",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_samples = model.sample(2000)\n",
    "conditional_correlation_matrix_train = model.compute_correlation_matrix(synthetic_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5309eeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#_ = plot_densities(synthetic_samples, x_lim=[-4,4], y_lim=[-4,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30faa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#_ = plot_metric_scatter(data=synthetic_samples,metric=conditional_correlation_matrix_train,metric_type=\"precision_matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308b2203",
   "metadata": {},
   "source": [
    "#### does the GTM identify the conditional independence structure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5b3615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20000, 10])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulated_data_test.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82487b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0 with var_row_num 1 and var_col_num 0.\n",
      "Processing row 6 with var_row_num 4 and var_col_num 0.\n",
      "Processing row 9 with var_row_num 4 and var_col_num 3.\n",
      "Processing row 3 with var_row_num 3 and var_col_num 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthiasherp/Desktop/phd_github_repositories/gtm/gtm/gtm_splines/bspline_prediction_vectorized.py:404: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3687.)\n",
      "  t=knots.T,\n",
      "/Users/matthiasherp/Desktop/phd_github_repositories/gtm/gtm/gtm_splines/bspline_prediction_vectorized.py:404: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3687.)\n",
      "  t=knots.T,\n",
      "/Users/matthiasherp/Desktop/phd_github_repositories/gtm/gtm/gtm_splines/bspline_prediction_vectorized.py:404: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3687.)\n",
      "  t=knots.T,\n",
      "/Users/matthiasherp/Desktop/phd_github_repositories/gtm/gtm/gtm_splines/bspline_prediction_vectorized.py:404: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3687.)\n",
      "  t=knots.T,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 4 with var_row_num 3 and var_col_num 1.\n",
      "Processing row 7 with var_row_num 4 and var_col_num 1.\n",
      "Processing row 1 with var_row_num 2 and var_col_num 0.\n",
      "Processing row 10 with var_row_num 5 and var_col_num 0.\n",
      "Processing row 5 with var_row_num 3 and var_col_num 2.\n",
      "Processing row 8 with var_row_num 4 and var_col_num 2.\n",
      "Processing row 2 with var_row_num 2 and var_col_num 1.\n",
      "Processing row 11 with var_row_num 5 and var_col_num 1.\n",
      "Processing row 12 with var_row_num 5 and var_col_num 2.\n",
      "Processing row 15 with var_row_num 6 and var_col_num 0.\n",
      "Processing row 18 with var_row_num 6 and var_col_num 3.\n",
      "Processing row 21 with var_row_num 7 and var_col_num 0.\n",
      "Processing row 13 with var_row_num 5 and var_col_num 3.\n",
      "Processing row 16 with var_row_num 6 and var_col_num 1.\n",
      "Processing row 19 with var_row_num 6 and var_col_num 4.\n",
      "Processing row 22 with var_row_num 7 and var_col_num 1.\n",
      "Processing row 14 with var_row_num 5 and var_col_num 4.\n",
      "Processing row 17 with var_row_num 6 and var_col_num 2.\n",
      "Processing row 20 with var_row_num 6 and var_col_num 5.\n",
      "Processing row 23 with var_row_num 7 and var_col_num 2.\n",
      "Processing row 24 with var_row_num 7 and var_col_num 3.\n",
      "Processing row 27 with var_row_num 7 and var_col_num 6.\n",
      "Processing row 30 with var_row_num 8 and var_col_num 2.\n",
      "Processing row 33 with var_row_num 8 and var_col_num 5.\n",
      "Processing row 25 with var_row_num 7 and var_col_num 4.\n",
      "Processing row 28 with var_row_num 8 and var_col_num 0.\n",
      "Processing row 31 with var_row_num 8 and var_col_num 3.\n",
      "Processing row 34 with var_row_num 8 and var_col_num 6.\n",
      "Processing row 26 with var_row_num 7 and var_col_num 5.\n",
      "Processing row 29 with var_row_num 8 and var_col_num 1.\n",
      "Processing row 32 with var_row_num 8 and var_col_num 4.\n",
      "Processing row 35 with var_row_num 8 and var_col_num 7.\n",
      "Processing row 36 with var_row_num 9 and var_col_num 0.\n",
      "Processing row 39 with var_row_num 9 and var_col_num 3.\n",
      "Processing row 42 with var_row_num 9 and var_col_num 6.\n",
      "Processing row 37 with var_row_num 9 and var_col_num 1.\n",
      "Processing row 40 with var_row_num 9 and var_col_num 4.\n",
      "Processing row 43 with var_row_num 9 and var_col_num 7.\n",
      "Processing row 38 with var_row_num 9 and var_col_num 2.\n",
      "Processing row 41 with var_row_num 9 and var_col_num 5.\n",
      "Processing row 44 with var_row_num 9 and var_col_num 8.\n",
      "Time taken: 54.85766005516052\n",
      "All rows processed.\n"
     ]
    }
   ],
   "source": [
    "conditional_independence_table = model.compute_conditional_independence_table(\n",
    "                                        y = simulated_data_test[:5000,:], #None\n",
    "                                        x = False,\n",
    "                                        evaluation_data_type = \"data\", #\"samples_from_model\",\n",
    "                                        num_processes=4,\n",
    "                                        sample_size = 1000,\n",
    "                                        num_points_quad=15,\n",
    "                                        optimized=True,\n",
    "                                        copula_only=False,\n",
    "                                        min_val=-6,\n",
    "                                        max_val=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b773375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 9)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merging together computed metrics for conditional independence table and actual structure of the simulation data for comparison\n",
    "merged = pd.merge(\n",
    "    conditional_independence_table,\n",
    "    df_true_structure_sub,\n",
    "    on=[\"var_row\", \"var_col\"] # or \"inner\", \"left\", depending on your needs\n",
    ")\n",
    "merged.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e083eea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var_row</th>\n",
       "      <th>var_col</th>\n",
       "      <th>precision_abs_mean</th>\n",
       "      <th>precision_square_mean</th>\n",
       "      <th>cond_correlation_abs_mean</th>\n",
       "      <th>cond_correlation_square_mean</th>\n",
       "      <th>kld</th>\n",
       "      <th>iae</th>\n",
       "      <th>dependence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>4.241405</td>\n",
       "      <td>20.432474</td>\n",
       "      <td>0.491744</td>\n",
       "      <td>0.270377</td>\n",
       "      <td>0.796407</td>\n",
       "      <td>2.292149</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>6.661479</td>\n",
       "      <td>45.859467</td>\n",
       "      <td>0.835930</td>\n",
       "      <td>0.699956</td>\n",
       "      <td>2.688869</td>\n",
       "      <td>1.930084</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>2.556951</td>\n",
       "      <td>7.173597</td>\n",
       "      <td>0.424617</td>\n",
       "      <td>0.183733</td>\n",
       "      <td>0.732047</td>\n",
       "      <td>1.625738</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2.491431</td>\n",
       "      <td>7.482593</td>\n",
       "      <td>0.425003</td>\n",
       "      <td>0.208859</td>\n",
       "      <td>0.605241</td>\n",
       "      <td>1.299844</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>4.028057</td>\n",
       "      <td>16.413799</td>\n",
       "      <td>0.632437</td>\n",
       "      <td>0.410131</td>\n",
       "      <td>1.381922</td>\n",
       "      <td>1.013068</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.818303</td>\n",
       "      <td>0.738617</td>\n",
       "      <td>0.213761</td>\n",
       "      <td>0.050335</td>\n",
       "      <td>0.233519</td>\n",
       "      <td>0.735718</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1.738819</td>\n",
       "      <td>3.156819</td>\n",
       "      <td>0.406261</td>\n",
       "      <td>0.169761</td>\n",
       "      <td>0.389400</td>\n",
       "      <td>0.714454</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2.834680</td>\n",
       "      <td>8.343678</td>\n",
       "      <td>0.605658</td>\n",
       "      <td>0.380874</td>\n",
       "      <td>0.762820</td>\n",
       "      <td>0.686833</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1.846897</td>\n",
       "      <td>5.609583</td>\n",
       "      <td>0.381413</td>\n",
       "      <td>0.187701</td>\n",
       "      <td>2.322047</td>\n",
       "      <td>0.582277</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1.864107</td>\n",
       "      <td>4.081544</td>\n",
       "      <td>0.618710</td>\n",
       "      <td>0.394771</td>\n",
       "      <td>0.720151</td>\n",
       "      <td>0.560343</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1.273742</td>\n",
       "      <td>1.787008</td>\n",
       "      <td>0.682809</td>\n",
       "      <td>0.478658</td>\n",
       "      <td>0.318496</td>\n",
       "      <td>0.435588</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.751294</td>\n",
       "      <td>0.573920</td>\n",
       "      <td>0.325636</td>\n",
       "      <td>0.107920</td>\n",
       "      <td>0.205051</td>\n",
       "      <td>0.356939</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.729561</td>\n",
       "      <td>0.559927</td>\n",
       "      <td>0.285139</td>\n",
       "      <td>0.087572</td>\n",
       "      <td>0.204988</td>\n",
       "      <td>0.356760</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1.187403</td>\n",
       "      <td>1.552322</td>\n",
       "      <td>0.249884</td>\n",
       "      <td>0.069879</td>\n",
       "      <td>0.268736</td>\n",
       "      <td>0.349577</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.706235</td>\n",
       "      <td>0.631904</td>\n",
       "      <td>0.219626</td>\n",
       "      <td>0.059692</td>\n",
       "      <td>0.186813</td>\n",
       "      <td>0.331516</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>0.771756</td>\n",
       "      <td>0.639639</td>\n",
       "      <td>0.273829</td>\n",
       "      <td>0.081102</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>0.315706</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.661436</td>\n",
       "      <td>0.467654</td>\n",
       "      <td>0.127408</td>\n",
       "      <td>0.017415</td>\n",
       "      <td>0.095686</td>\n",
       "      <td>0.289866</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.210283</td>\n",
       "      <td>0.069348</td>\n",
       "      <td>0.067771</td>\n",
       "      <td>0.007398</td>\n",
       "      <td>0.023266</td>\n",
       "      <td>0.226964</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.508590</td>\n",
       "      <td>0.348244</td>\n",
       "      <td>0.157288</td>\n",
       "      <td>0.034611</td>\n",
       "      <td>0.105225</td>\n",
       "      <td>0.220134</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.256672</td>\n",
       "      <td>0.107731</td>\n",
       "      <td>0.065516</td>\n",
       "      <td>0.006715</td>\n",
       "      <td>0.032563</td>\n",
       "      <td>0.194241</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.297285</td>\n",
       "      <td>0.115763</td>\n",
       "      <td>0.082333</td>\n",
       "      <td>0.008451</td>\n",
       "      <td>0.006889</td>\n",
       "      <td>0.180789</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.256004</td>\n",
       "      <td>0.130830</td>\n",
       "      <td>0.076673</td>\n",
       "      <td>0.013075</td>\n",
       "      <td>0.074879</td>\n",
       "      <td>0.178356</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>0.266345</td>\n",
       "      <td>0.117526</td>\n",
       "      <td>0.066841</td>\n",
       "      <td>0.007155</td>\n",
       "      <td>0.080181</td>\n",
       "      <td>0.168708</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.414069</td>\n",
       "      <td>0.188731</td>\n",
       "      <td>0.140820</td>\n",
       "      <td>0.021784</td>\n",
       "      <td>0.052756</td>\n",
       "      <td>0.163591</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.104984</td>\n",
       "      <td>0.020694</td>\n",
       "      <td>0.043783</td>\n",
       "      <td>0.003407</td>\n",
       "      <td>0.013733</td>\n",
       "      <td>0.160266</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.184711</td>\n",
       "      <td>0.055430</td>\n",
       "      <td>0.051490</td>\n",
       "      <td>0.004107</td>\n",
       "      <td>0.033027</td>\n",
       "      <td>0.152177</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0.109928</td>\n",
       "      <td>0.018514</td>\n",
       "      <td>0.057653</td>\n",
       "      <td>0.004910</td>\n",
       "      <td>0.019881</td>\n",
       "      <td>0.119155</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.157967</td>\n",
       "      <td>0.033460</td>\n",
       "      <td>0.061412</td>\n",
       "      <td>0.004954</td>\n",
       "      <td>0.022417</td>\n",
       "      <td>0.113821</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0.134211</td>\n",
       "      <td>0.047377</td>\n",
       "      <td>0.029483</td>\n",
       "      <td>0.002138</td>\n",
       "      <td>0.036547</td>\n",
       "      <td>0.112468</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.086257</td>\n",
       "      <td>0.012298</td>\n",
       "      <td>0.038861</td>\n",
       "      <td>0.002438</td>\n",
       "      <td>0.016337</td>\n",
       "      <td>0.097751</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0.084028</td>\n",
       "      <td>0.009545</td>\n",
       "      <td>0.034267</td>\n",
       "      <td>0.001598</td>\n",
       "      <td>0.020870</td>\n",
       "      <td>0.093092</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.085397</td>\n",
       "      <td>0.013423</td>\n",
       "      <td>0.032802</td>\n",
       "      <td>0.002008</td>\n",
       "      <td>0.002388</td>\n",
       "      <td>0.078173</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.171021</td>\n",
       "      <td>0.036514</td>\n",
       "      <td>0.080217</td>\n",
       "      <td>0.007879</td>\n",
       "      <td>0.008653</td>\n",
       "      <td>0.076907</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.047781</td>\n",
       "      <td>0.003666</td>\n",
       "      <td>0.028111</td>\n",
       "      <td>0.001322</td>\n",
       "      <td>-0.000260</td>\n",
       "      <td>0.068863</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.099351</td>\n",
       "      <td>0.016216</td>\n",
       "      <td>0.044537</td>\n",
       "      <td>0.002977</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.060742</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.056580</td>\n",
       "      <td>0.005371</td>\n",
       "      <td>0.029296</td>\n",
       "      <td>0.001398</td>\n",
       "      <td>0.010360</td>\n",
       "      <td>0.059612</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.063003</td>\n",
       "      <td>0.006844</td>\n",
       "      <td>0.024715</td>\n",
       "      <td>0.000974</td>\n",
       "      <td>-0.000140</td>\n",
       "      <td>0.052314</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.051090</td>\n",
       "      <td>0.004062</td>\n",
       "      <td>0.039021</td>\n",
       "      <td>0.002310</td>\n",
       "      <td>0.000525</td>\n",
       "      <td>0.042773</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.079983</td>\n",
       "      <td>0.007950</td>\n",
       "      <td>0.051173</td>\n",
       "      <td>0.003317</td>\n",
       "      <td>0.000549</td>\n",
       "      <td>0.038754</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.063161</td>\n",
       "      <td>0.005009</td>\n",
       "      <td>0.055128</td>\n",
       "      <td>0.003816</td>\n",
       "      <td>0.000310</td>\n",
       "      <td>0.036430</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.044229</td>\n",
       "      <td>0.003207</td>\n",
       "      <td>0.027559</td>\n",
       "      <td>0.001179</td>\n",
       "      <td>0.000712</td>\n",
       "      <td>0.035230</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.045430</td>\n",
       "      <td>0.003432</td>\n",
       "      <td>0.022543</td>\n",
       "      <td>0.000860</td>\n",
       "      <td>-0.002614</td>\n",
       "      <td>0.034831</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>0.034130</td>\n",
       "      <td>0.001926</td>\n",
       "      <td>0.019908</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>-0.002087</td>\n",
       "      <td>0.034069</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.029306</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>0.020733</td>\n",
       "      <td>0.000722</td>\n",
       "      <td>0.003382</td>\n",
       "      <td>0.031600</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>0.038893</td>\n",
       "      <td>0.002234</td>\n",
       "      <td>0.026599</td>\n",
       "      <td>0.001045</td>\n",
       "      <td>-0.000793</td>\n",
       "      <td>0.030475</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    var_row  var_col  precision_abs_mean  precision_square_mean  \\\n",
       "26        7        5            4.241405              20.432474   \n",
       "22        7        1            6.661479              45.859467   \n",
       "24        7        3            2.556951               7.173597   \n",
       "11        5        1            2.491431               7.482593   \n",
       "25        7        4            4.028057              16.413799   \n",
       "4         3        1            0.818303               0.738617   \n",
       "7         4        1            1.738819               3.156819   \n",
       "14        5        4            2.834680               8.343678   \n",
       "13        5        3            1.846897               5.609583   \n",
       "19        6        4            1.864107               4.081544   \n",
       "30        8        2            1.273742               1.787008   \n",
       "0         1        0            0.751294               0.573920   \n",
       "37        9        1            0.729561               0.559927   \n",
       "23        7        2            1.187403               1.552322   \n",
       "2         2        1            0.706235               0.631904   \n",
       "41        9        5            0.771756               0.639639   \n",
       "27        7        6            0.661436               0.467654   \n",
       "9         4        3            0.210283               0.069348   \n",
       "33        8        5            0.508590               0.348244   \n",
       "20        6        5            0.256672               0.107731   \n",
       "16        6        1            0.297285               0.115763   \n",
       "12        5        2            0.256004               0.130830   \n",
       "43        9        7            0.266345               0.117526   \n",
       "29        8        1            0.414069               0.188731   \n",
       "5         3        2            0.104984               0.020694   \n",
       "21        7        0            0.184711               0.055430   \n",
       "39        9        3            0.109928               0.018514   \n",
       "10        5        0            0.157967               0.033460   \n",
       "35        8        7            0.134211               0.047377   \n",
       "31        8        3            0.086257               0.012298   \n",
       "18        6        3            0.084028               0.009545   \n",
       "8         4        2            0.085397               0.013423   \n",
       "40        9        4            0.171021               0.036514   \n",
       "3         3        0            0.047781               0.003666   \n",
       "17        6        2            0.099351               0.016216   \n",
       "6         4        0            0.056580               0.005371   \n",
       "32        8        4            0.063003               0.006844   \n",
       "28        8        0            0.051090               0.004062   \n",
       "38        9        2            0.079983               0.007950   \n",
       "36        9        0            0.063161               0.005009   \n",
       "15        6        0            0.044229               0.003207   \n",
       "34        8        6            0.045430               0.003432   \n",
       "42        9        6            0.034130               0.001926   \n",
       "1         2        0            0.029306               0.001443   \n",
       "44        9        8            0.038893               0.002234   \n",
       "\n",
       "    cond_correlation_abs_mean  cond_correlation_square_mean       kld  \\\n",
       "26                   0.491744                      0.270377  0.796407   \n",
       "22                   0.835930                      0.699956  2.688869   \n",
       "24                   0.424617                      0.183733  0.732047   \n",
       "11                   0.425003                      0.208859  0.605241   \n",
       "25                   0.632437                      0.410131  1.381922   \n",
       "4                    0.213761                      0.050335  0.233519   \n",
       "7                    0.406261                      0.169761  0.389400   \n",
       "14                   0.605658                      0.380874  0.762820   \n",
       "13                   0.381413                      0.187701  2.322047   \n",
       "19                   0.618710                      0.394771  0.720151   \n",
       "30                   0.682809                      0.478658  0.318496   \n",
       "0                    0.325636                      0.107920  0.205051   \n",
       "37                   0.285139                      0.087572  0.204988   \n",
       "23                   0.249884                      0.069879  0.268736   \n",
       "2                    0.219626                      0.059692  0.186813   \n",
       "41                   0.273829                      0.081102  0.170213   \n",
       "27                   0.127408                      0.017415  0.095686   \n",
       "9                    0.067771                      0.007398  0.023266   \n",
       "33                   0.157288                      0.034611  0.105225   \n",
       "20                   0.065516                      0.006715  0.032563   \n",
       "16                   0.082333                      0.008451  0.006889   \n",
       "12                   0.076673                      0.013075  0.074879   \n",
       "43                   0.066841                      0.007155  0.080181   \n",
       "29                   0.140820                      0.021784  0.052756   \n",
       "5                    0.043783                      0.003407  0.013733   \n",
       "21                   0.051490                      0.004107  0.033027   \n",
       "39                   0.057653                      0.004910  0.019881   \n",
       "10                   0.061412                      0.004954  0.022417   \n",
       "35                   0.029483                      0.002138  0.036547   \n",
       "31                   0.038861                      0.002438  0.016337   \n",
       "18                   0.034267                      0.001598  0.020870   \n",
       "8                    0.032802                      0.002008  0.002388   \n",
       "40                   0.080217                      0.007879  0.008653   \n",
       "3                    0.028111                      0.001322 -0.000260   \n",
       "17                   0.044537                      0.002977  0.000172   \n",
       "6                    0.029296                      0.001398  0.010360   \n",
       "32                   0.024715                      0.000974 -0.000140   \n",
       "28                   0.039021                      0.002310  0.000525   \n",
       "38                   0.051173                      0.003317  0.000549   \n",
       "36                   0.055128                      0.003816  0.000310   \n",
       "15                   0.027559                      0.001179  0.000712   \n",
       "34                   0.022543                      0.000860 -0.002614   \n",
       "42                   0.019908                      0.000663 -0.002087   \n",
       "1                    0.020733                      0.000722  0.003382   \n",
       "44                   0.026599                      0.001045 -0.000793   \n",
       "\n",
       "         iae  dependence  \n",
       "26  2.292149           1  \n",
       "22  1.930084           1  \n",
       "24  1.625738           1  \n",
       "11  1.299844           1  \n",
       "25  1.013068           1  \n",
       "4   0.735718           0  \n",
       "7   0.714454           0  \n",
       "14  0.686833           1  \n",
       "13  0.582277           1  \n",
       "19  0.560343           1  \n",
       "30  0.435588           1  \n",
       "0   0.356939           0  \n",
       "37  0.356760           1  \n",
       "23  0.349577           0  \n",
       "2   0.331516           1  \n",
       "41  0.315706           1  \n",
       "27  0.289866           0  \n",
       "9   0.226964           0  \n",
       "33  0.220134           1  \n",
       "20  0.194241           1  \n",
       "16  0.180789           0  \n",
       "12  0.178356           1  \n",
       "43  0.168708           0  \n",
       "29  0.163591           0  \n",
       "5   0.160266           0  \n",
       "21  0.152177           1  \n",
       "39  0.119155           0  \n",
       "10  0.113821           1  \n",
       "35  0.112468           0  \n",
       "31  0.097751           0  \n",
       "18  0.093092           0  \n",
       "8   0.078173           0  \n",
       "40  0.076907           0  \n",
       "3   0.068863           0  \n",
       "17  0.060742           0  \n",
       "6   0.059612           0  \n",
       "32  0.052314           0  \n",
       "28  0.042773           0  \n",
       "38  0.038754           0  \n",
       "36  0.036430           0  \n",
       "15  0.035230           0  \n",
       "34  0.034831           0  \n",
       "42  0.034069           0  \n",
       "1   0.031600           0  \n",
       "44  0.030475           0  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.sort_values(\"iae\",ascending=False) #.sort_values(\"abs_mean\",ascending=False) #.sort_values(\"iae\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fc7423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var_row</th>\n",
       "      <th>var_col</th>\n",
       "      <th>precision_abs_mean</th>\n",
       "      <th>precision_square_mean</th>\n",
       "      <th>cond_correlation_abs_mean</th>\n",
       "      <th>cond_correlation_square_mean</th>\n",
       "      <th>kld</th>\n",
       "      <th>iae</th>\n",
       "      <th>dependence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>6.661479</td>\n",
       "      <td>45.859467</td>\n",
       "      <td>0.835930</td>\n",
       "      <td>0.699956</td>\n",
       "      <td>2.688869</td>\n",
       "      <td>1.930084</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1.273742</td>\n",
       "      <td>1.787008</td>\n",
       "      <td>0.682809</td>\n",
       "      <td>0.478658</td>\n",
       "      <td>0.318496</td>\n",
       "      <td>0.435588</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>4.028057</td>\n",
       "      <td>16.413799</td>\n",
       "      <td>0.632437</td>\n",
       "      <td>0.410131</td>\n",
       "      <td>1.381922</td>\n",
       "      <td>1.013068</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1.864107</td>\n",
       "      <td>4.081544</td>\n",
       "      <td>0.618710</td>\n",
       "      <td>0.394771</td>\n",
       "      <td>0.720151</td>\n",
       "      <td>0.560343</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2.834680</td>\n",
       "      <td>8.343678</td>\n",
       "      <td>0.605658</td>\n",
       "      <td>0.380874</td>\n",
       "      <td>0.762820</td>\n",
       "      <td>0.686833</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>4.241405</td>\n",
       "      <td>20.432474</td>\n",
       "      <td>0.491744</td>\n",
       "      <td>0.270377</td>\n",
       "      <td>0.796407</td>\n",
       "      <td>2.292149</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2.491431</td>\n",
       "      <td>7.482593</td>\n",
       "      <td>0.425003</td>\n",
       "      <td>0.208859</td>\n",
       "      <td>0.605241</td>\n",
       "      <td>1.299844</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>2.556951</td>\n",
       "      <td>7.173597</td>\n",
       "      <td>0.424617</td>\n",
       "      <td>0.183733</td>\n",
       "      <td>0.732047</td>\n",
       "      <td>1.625738</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1.738819</td>\n",
       "      <td>3.156819</td>\n",
       "      <td>0.406261</td>\n",
       "      <td>0.169761</td>\n",
       "      <td>0.389400</td>\n",
       "      <td>0.714454</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1.846897</td>\n",
       "      <td>5.609583</td>\n",
       "      <td>0.381413</td>\n",
       "      <td>0.187701</td>\n",
       "      <td>2.322047</td>\n",
       "      <td>0.582277</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.751294</td>\n",
       "      <td>0.573920</td>\n",
       "      <td>0.325636</td>\n",
       "      <td>0.107920</td>\n",
       "      <td>0.205051</td>\n",
       "      <td>0.356939</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.729561</td>\n",
       "      <td>0.559927</td>\n",
       "      <td>0.285139</td>\n",
       "      <td>0.087572</td>\n",
       "      <td>0.204988</td>\n",
       "      <td>0.356760</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>0.771756</td>\n",
       "      <td>0.639639</td>\n",
       "      <td>0.273829</td>\n",
       "      <td>0.081102</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>0.315706</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1.187403</td>\n",
       "      <td>1.552322</td>\n",
       "      <td>0.249884</td>\n",
       "      <td>0.069879</td>\n",
       "      <td>0.268736</td>\n",
       "      <td>0.349577</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.706235</td>\n",
       "      <td>0.631904</td>\n",
       "      <td>0.219626</td>\n",
       "      <td>0.059692</td>\n",
       "      <td>0.186813</td>\n",
       "      <td>0.331516</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.818303</td>\n",
       "      <td>0.738617</td>\n",
       "      <td>0.213761</td>\n",
       "      <td>0.050335</td>\n",
       "      <td>0.233519</td>\n",
       "      <td>0.735718</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.508590</td>\n",
       "      <td>0.348244</td>\n",
       "      <td>0.157288</td>\n",
       "      <td>0.034611</td>\n",
       "      <td>0.105225</td>\n",
       "      <td>0.220134</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.414069</td>\n",
       "      <td>0.188731</td>\n",
       "      <td>0.140820</td>\n",
       "      <td>0.021784</td>\n",
       "      <td>0.052756</td>\n",
       "      <td>0.163591</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.661436</td>\n",
       "      <td>0.467654</td>\n",
       "      <td>0.127408</td>\n",
       "      <td>0.017415</td>\n",
       "      <td>0.095686</td>\n",
       "      <td>0.289866</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.297285</td>\n",
       "      <td>0.115763</td>\n",
       "      <td>0.082333</td>\n",
       "      <td>0.008451</td>\n",
       "      <td>0.006889</td>\n",
       "      <td>0.180789</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.171021</td>\n",
       "      <td>0.036514</td>\n",
       "      <td>0.080217</td>\n",
       "      <td>0.007879</td>\n",
       "      <td>0.008653</td>\n",
       "      <td>0.076907</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.256004</td>\n",
       "      <td>0.130830</td>\n",
       "      <td>0.076673</td>\n",
       "      <td>0.013075</td>\n",
       "      <td>0.074879</td>\n",
       "      <td>0.178356</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.210283</td>\n",
       "      <td>0.069348</td>\n",
       "      <td>0.067771</td>\n",
       "      <td>0.007398</td>\n",
       "      <td>0.023266</td>\n",
       "      <td>0.226964</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>0.266345</td>\n",
       "      <td>0.117526</td>\n",
       "      <td>0.066841</td>\n",
       "      <td>0.007155</td>\n",
       "      <td>0.080181</td>\n",
       "      <td>0.168708</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.256672</td>\n",
       "      <td>0.107731</td>\n",
       "      <td>0.065516</td>\n",
       "      <td>0.006715</td>\n",
       "      <td>0.032563</td>\n",
       "      <td>0.194241</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.157967</td>\n",
       "      <td>0.033460</td>\n",
       "      <td>0.061412</td>\n",
       "      <td>0.004954</td>\n",
       "      <td>0.022417</td>\n",
       "      <td>0.113821</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0.109928</td>\n",
       "      <td>0.018514</td>\n",
       "      <td>0.057653</td>\n",
       "      <td>0.004910</td>\n",
       "      <td>0.019881</td>\n",
       "      <td>0.119155</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.063161</td>\n",
       "      <td>0.005009</td>\n",
       "      <td>0.055128</td>\n",
       "      <td>0.003816</td>\n",
       "      <td>0.000310</td>\n",
       "      <td>0.036430</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.184711</td>\n",
       "      <td>0.055430</td>\n",
       "      <td>0.051490</td>\n",
       "      <td>0.004107</td>\n",
       "      <td>0.033027</td>\n",
       "      <td>0.152177</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.079983</td>\n",
       "      <td>0.007950</td>\n",
       "      <td>0.051173</td>\n",
       "      <td>0.003317</td>\n",
       "      <td>0.000549</td>\n",
       "      <td>0.038754</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.099351</td>\n",
       "      <td>0.016216</td>\n",
       "      <td>0.044537</td>\n",
       "      <td>0.002977</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.060742</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.104984</td>\n",
       "      <td>0.020694</td>\n",
       "      <td>0.043783</td>\n",
       "      <td>0.003407</td>\n",
       "      <td>0.013733</td>\n",
       "      <td>0.160266</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.051090</td>\n",
       "      <td>0.004062</td>\n",
       "      <td>0.039021</td>\n",
       "      <td>0.002310</td>\n",
       "      <td>0.000525</td>\n",
       "      <td>0.042773</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.086257</td>\n",
       "      <td>0.012298</td>\n",
       "      <td>0.038861</td>\n",
       "      <td>0.002438</td>\n",
       "      <td>0.016337</td>\n",
       "      <td>0.097751</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0.084028</td>\n",
       "      <td>0.009545</td>\n",
       "      <td>0.034267</td>\n",
       "      <td>0.001598</td>\n",
       "      <td>0.020870</td>\n",
       "      <td>0.093092</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.085397</td>\n",
       "      <td>0.013423</td>\n",
       "      <td>0.032802</td>\n",
       "      <td>0.002008</td>\n",
       "      <td>0.002388</td>\n",
       "      <td>0.078173</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0.134211</td>\n",
       "      <td>0.047377</td>\n",
       "      <td>0.029483</td>\n",
       "      <td>0.002138</td>\n",
       "      <td>0.036547</td>\n",
       "      <td>0.112468</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.056580</td>\n",
       "      <td>0.005371</td>\n",
       "      <td>0.029296</td>\n",
       "      <td>0.001398</td>\n",
       "      <td>0.010360</td>\n",
       "      <td>0.059612</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.047781</td>\n",
       "      <td>0.003666</td>\n",
       "      <td>0.028111</td>\n",
       "      <td>0.001322</td>\n",
       "      <td>-0.000260</td>\n",
       "      <td>0.068863</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.044229</td>\n",
       "      <td>0.003207</td>\n",
       "      <td>0.027559</td>\n",
       "      <td>0.001179</td>\n",
       "      <td>0.000712</td>\n",
       "      <td>0.035230</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>0.038893</td>\n",
       "      <td>0.002234</td>\n",
       "      <td>0.026599</td>\n",
       "      <td>0.001045</td>\n",
       "      <td>-0.000793</td>\n",
       "      <td>0.030475</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.063003</td>\n",
       "      <td>0.006844</td>\n",
       "      <td>0.024715</td>\n",
       "      <td>0.000974</td>\n",
       "      <td>-0.000140</td>\n",
       "      <td>0.052314</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.045430</td>\n",
       "      <td>0.003432</td>\n",
       "      <td>0.022543</td>\n",
       "      <td>0.000860</td>\n",
       "      <td>-0.002614</td>\n",
       "      <td>0.034831</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.029306</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>0.020733</td>\n",
       "      <td>0.000722</td>\n",
       "      <td>0.003382</td>\n",
       "      <td>0.031600</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>0.034130</td>\n",
       "      <td>0.001926</td>\n",
       "      <td>0.019908</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>-0.002087</td>\n",
       "      <td>0.034069</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    var_row  var_col  precision_abs_mean  precision_square_mean  \\\n",
       "22        7        1            6.661479              45.859467   \n",
       "30        8        2            1.273742               1.787008   \n",
       "25        7        4            4.028057              16.413799   \n",
       "19        6        4            1.864107               4.081544   \n",
       "14        5        4            2.834680               8.343678   \n",
       "26        7        5            4.241405              20.432474   \n",
       "11        5        1            2.491431               7.482593   \n",
       "24        7        3            2.556951               7.173597   \n",
       "7         4        1            1.738819               3.156819   \n",
       "13        5        3            1.846897               5.609583   \n",
       "0         1        0            0.751294               0.573920   \n",
       "37        9        1            0.729561               0.559927   \n",
       "41        9        5            0.771756               0.639639   \n",
       "23        7        2            1.187403               1.552322   \n",
       "2         2        1            0.706235               0.631904   \n",
       "4         3        1            0.818303               0.738617   \n",
       "33        8        5            0.508590               0.348244   \n",
       "29        8        1            0.414069               0.188731   \n",
       "27        7        6            0.661436               0.467654   \n",
       "16        6        1            0.297285               0.115763   \n",
       "40        9        4            0.171021               0.036514   \n",
       "12        5        2            0.256004               0.130830   \n",
       "9         4        3            0.210283               0.069348   \n",
       "43        9        7            0.266345               0.117526   \n",
       "20        6        5            0.256672               0.107731   \n",
       "10        5        0            0.157967               0.033460   \n",
       "39        9        3            0.109928               0.018514   \n",
       "36        9        0            0.063161               0.005009   \n",
       "21        7        0            0.184711               0.055430   \n",
       "38        9        2            0.079983               0.007950   \n",
       "17        6        2            0.099351               0.016216   \n",
       "5         3        2            0.104984               0.020694   \n",
       "28        8        0            0.051090               0.004062   \n",
       "31        8        3            0.086257               0.012298   \n",
       "18        6        3            0.084028               0.009545   \n",
       "8         4        2            0.085397               0.013423   \n",
       "35        8        7            0.134211               0.047377   \n",
       "6         4        0            0.056580               0.005371   \n",
       "3         3        0            0.047781               0.003666   \n",
       "15        6        0            0.044229               0.003207   \n",
       "44        9        8            0.038893               0.002234   \n",
       "32        8        4            0.063003               0.006844   \n",
       "34        8        6            0.045430               0.003432   \n",
       "1         2        0            0.029306               0.001443   \n",
       "42        9        6            0.034130               0.001926   \n",
       "\n",
       "    cond_correlation_abs_mean  cond_correlation_square_mean       kld  \\\n",
       "22                   0.835930                      0.699956  2.688869   \n",
       "30                   0.682809                      0.478658  0.318496   \n",
       "25                   0.632437                      0.410131  1.381922   \n",
       "19                   0.618710                      0.394771  0.720151   \n",
       "14                   0.605658                      0.380874  0.762820   \n",
       "26                   0.491744                      0.270377  0.796407   \n",
       "11                   0.425003                      0.208859  0.605241   \n",
       "24                   0.424617                      0.183733  0.732047   \n",
       "7                    0.406261                      0.169761  0.389400   \n",
       "13                   0.381413                      0.187701  2.322047   \n",
       "0                    0.325636                      0.107920  0.205051   \n",
       "37                   0.285139                      0.087572  0.204988   \n",
       "41                   0.273829                      0.081102  0.170213   \n",
       "23                   0.249884                      0.069879  0.268736   \n",
       "2                    0.219626                      0.059692  0.186813   \n",
       "4                    0.213761                      0.050335  0.233519   \n",
       "33                   0.157288                      0.034611  0.105225   \n",
       "29                   0.140820                      0.021784  0.052756   \n",
       "27                   0.127408                      0.017415  0.095686   \n",
       "16                   0.082333                      0.008451  0.006889   \n",
       "40                   0.080217                      0.007879  0.008653   \n",
       "12                   0.076673                      0.013075  0.074879   \n",
       "9                    0.067771                      0.007398  0.023266   \n",
       "43                   0.066841                      0.007155  0.080181   \n",
       "20                   0.065516                      0.006715  0.032563   \n",
       "10                   0.061412                      0.004954  0.022417   \n",
       "39                   0.057653                      0.004910  0.019881   \n",
       "36                   0.055128                      0.003816  0.000310   \n",
       "21                   0.051490                      0.004107  0.033027   \n",
       "38                   0.051173                      0.003317  0.000549   \n",
       "17                   0.044537                      0.002977  0.000172   \n",
       "5                    0.043783                      0.003407  0.013733   \n",
       "28                   0.039021                      0.002310  0.000525   \n",
       "31                   0.038861                      0.002438  0.016337   \n",
       "18                   0.034267                      0.001598  0.020870   \n",
       "8                    0.032802                      0.002008  0.002388   \n",
       "35                   0.029483                      0.002138  0.036547   \n",
       "6                    0.029296                      0.001398  0.010360   \n",
       "3                    0.028111                      0.001322 -0.000260   \n",
       "15                   0.027559                      0.001179  0.000712   \n",
       "44                   0.026599                      0.001045 -0.000793   \n",
       "32                   0.024715                      0.000974 -0.000140   \n",
       "34                   0.022543                      0.000860 -0.002614   \n",
       "1                    0.020733                      0.000722  0.003382   \n",
       "42                   0.019908                      0.000663 -0.002087   \n",
       "\n",
       "         iae  dependence  \n",
       "22  1.930084           1  \n",
       "30  0.435588           1  \n",
       "25  1.013068           1  \n",
       "19  0.560343           1  \n",
       "14  0.686833           1  \n",
       "26  2.292149           1  \n",
       "11  1.299844           1  \n",
       "24  1.625738           1  \n",
       "7   0.714454           0  \n",
       "13  0.582277           1  \n",
       "0   0.356939           0  \n",
       "37  0.356760           1  \n",
       "41  0.315706           1  \n",
       "23  0.349577           0  \n",
       "2   0.331516           1  \n",
       "4   0.735718           0  \n",
       "33  0.220134           1  \n",
       "29  0.163591           0  \n",
       "27  0.289866           0  \n",
       "16  0.180789           0  \n",
       "40  0.076907           0  \n",
       "12  0.178356           1  \n",
       "9   0.226964           0  \n",
       "43  0.168708           0  \n",
       "20  0.194241           1  \n",
       "10  0.113821           1  \n",
       "39  0.119155           0  \n",
       "36  0.036430           0  \n",
       "21  0.152177           1  \n",
       "38  0.038754           0  \n",
       "17  0.060742           0  \n",
       "5   0.160266           0  \n",
       "28  0.042773           0  \n",
       "31  0.097751           0  \n",
       "18  0.093092           0  \n",
       "8   0.078173           0  \n",
       "35  0.112468           0  \n",
       "6   0.059612           0  \n",
       "3   0.068863           0  \n",
       "15  0.035230           0  \n",
       "44  0.030475           0  \n",
       "32  0.052314           0  \n",
       "34  0.034831           0  \n",
       "1   0.031600           0  \n",
       "42  0.034069           0  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.sort_values(\"cond_correlation_abs_mean\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dfe4c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var_row</th>\n",
       "      <th>var_col</th>\n",
       "      <th>precision_abs_mean</th>\n",
       "      <th>precision_square_mean</th>\n",
       "      <th>cond_correlation_abs_mean</th>\n",
       "      <th>cond_correlation_square_mean</th>\n",
       "      <th>kld</th>\n",
       "      <th>iae</th>\n",
       "      <th>dependence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>var_row</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.468889</td>\n",
       "      <td>-0.139536</td>\n",
       "      <td>-0.160982</td>\n",
       "      <td>-0.092313</td>\n",
       "      <td>-0.102770</td>\n",
       "      <td>-0.204142</td>\n",
       "      <td>-0.257092</td>\n",
       "      <td>-0.051740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_col</th>\n",
       "      <td>0.468889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.111495</td>\n",
       "      <td>0.118355</td>\n",
       "      <td>0.015918</td>\n",
       "      <td>0.024444</td>\n",
       "      <td>0.068203</td>\n",
       "      <td>0.059544</td>\n",
       "      <td>0.067797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision_abs_mean</th>\n",
       "      <td>-0.139536</td>\n",
       "      <td>0.111495</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997365</td>\n",
       "      <td>0.968116</td>\n",
       "      <td>0.972596</td>\n",
       "      <td>0.952042</td>\n",
       "      <td>0.975494</td>\n",
       "      <td>0.656440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision_square_mean</th>\n",
       "      <td>-0.160982</td>\n",
       "      <td>0.118355</td>\n",
       "      <td>0.997365</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.958762</td>\n",
       "      <td>0.965744</td>\n",
       "      <td>0.957576</td>\n",
       "      <td>0.975626</td>\n",
       "      <td>0.663498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cond_correlation_abs_mean</th>\n",
       "      <td>-0.092313</td>\n",
       "      <td>0.015918</td>\n",
       "      <td>0.968116</td>\n",
       "      <td>0.958762</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998024</td>\n",
       "      <td>0.911858</td>\n",
       "      <td>0.930040</td>\n",
       "      <td>0.656440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cond_correlation_square_mean</th>\n",
       "      <td>-0.102770</td>\n",
       "      <td>0.024444</td>\n",
       "      <td>0.972596</td>\n",
       "      <td>0.965744</td>\n",
       "      <td>0.998024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923452</td>\n",
       "      <td>0.936891</td>\n",
       "      <td>0.670557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kld</th>\n",
       "      <td>-0.204142</td>\n",
       "      <td>0.068203</td>\n",
       "      <td>0.952042</td>\n",
       "      <td>0.957576</td>\n",
       "      <td>0.911858</td>\n",
       "      <td>0.923452</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.947958</td>\n",
       "      <td>0.670557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iae</th>\n",
       "      <td>-0.257092</td>\n",
       "      <td>0.059544</td>\n",
       "      <td>0.975494</td>\n",
       "      <td>0.975626</td>\n",
       "      <td>0.930040</td>\n",
       "      <td>0.936891</td>\n",
       "      <td>0.947958</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.631735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dependence</th>\n",
       "      <td>-0.051740</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.656440</td>\n",
       "      <td>0.663498</td>\n",
       "      <td>0.656440</td>\n",
       "      <td>0.670557</td>\n",
       "      <td>0.670557</td>\n",
       "      <td>0.631735</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               var_row   var_col  precision_abs_mean  \\\n",
       "var_row                       1.000000  0.468889           -0.139536   \n",
       "var_col                       0.468889  1.000000            0.111495   \n",
       "precision_abs_mean           -0.139536  0.111495            1.000000   \n",
       "precision_square_mean        -0.160982  0.118355            0.997365   \n",
       "cond_correlation_abs_mean    -0.092313  0.015918            0.968116   \n",
       "cond_correlation_square_mean -0.102770  0.024444            0.972596   \n",
       "kld                          -0.204142  0.068203            0.952042   \n",
       "iae                          -0.257092  0.059544            0.975494   \n",
       "dependence                   -0.051740  0.067797            0.656440   \n",
       "\n",
       "                              precision_square_mean  \\\n",
       "var_row                                   -0.160982   \n",
       "var_col                                    0.118355   \n",
       "precision_abs_mean                         0.997365   \n",
       "precision_square_mean                      1.000000   \n",
       "cond_correlation_abs_mean                  0.958762   \n",
       "cond_correlation_square_mean               0.965744   \n",
       "kld                                        0.957576   \n",
       "iae                                        0.975626   \n",
       "dependence                                 0.663498   \n",
       "\n",
       "                              cond_correlation_abs_mean  \\\n",
       "var_row                                       -0.092313   \n",
       "var_col                                        0.015918   \n",
       "precision_abs_mean                             0.968116   \n",
       "precision_square_mean                          0.958762   \n",
       "cond_correlation_abs_mean                      1.000000   \n",
       "cond_correlation_square_mean                   0.998024   \n",
       "kld                                            0.911858   \n",
       "iae                                            0.930040   \n",
       "dependence                                     0.656440   \n",
       "\n",
       "                              cond_correlation_square_mean       kld  \\\n",
       "var_row                                          -0.102770 -0.204142   \n",
       "var_col                                           0.024444  0.068203   \n",
       "precision_abs_mean                                0.972596  0.952042   \n",
       "precision_square_mean                             0.965744  0.957576   \n",
       "cond_correlation_abs_mean                         0.998024  0.911858   \n",
       "cond_correlation_square_mean                      1.000000  0.923452   \n",
       "kld                                               0.923452  1.000000   \n",
       "iae                                               0.936891  0.947958   \n",
       "dependence                                        0.670557  0.670557   \n",
       "\n",
       "                                   iae  dependence  \n",
       "var_row                      -0.257092   -0.051740  \n",
       "var_col                       0.059544    0.067797  \n",
       "precision_abs_mean            0.975494    0.656440  \n",
       "precision_square_mean         0.975626    0.663498  \n",
       "cond_correlation_abs_mean     0.930040    0.656440  \n",
       "cond_correlation_square_mean  0.936891    0.670557  \n",
       "kld                           0.947958    0.670557  \n",
       "iae                           1.000000    0.631735  \n",
       "dependence                    0.631735    1.000000  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.corr(\"spearman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531c7fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var_row</th>\n",
       "      <th>var_col</th>\n",
       "      <th>precision_abs_mean</th>\n",
       "      <th>precision_square_mean</th>\n",
       "      <th>cond_correlation_abs_mean</th>\n",
       "      <th>cond_correlation_square_mean</th>\n",
       "      <th>kld</th>\n",
       "      <th>iae</th>\n",
       "      <th>dependence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>var_row</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.021973</td>\n",
       "      <td>0.029770</td>\n",
       "      <td>-0.052468</td>\n",
       "      <td>-0.006787</td>\n",
       "      <td>-0.051600</td>\n",
       "      <td>-0.064045</td>\n",
       "      <td>0.006910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_col</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.019489</td>\n",
       "      <td>-0.008769</td>\n",
       "      <td>-0.021231</td>\n",
       "      <td>-0.009877</td>\n",
       "      <td>-0.003765</td>\n",
       "      <td>0.018798</td>\n",
       "      <td>0.013820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision_abs_mean</th>\n",
       "      <td>-0.021973</td>\n",
       "      <td>0.019489</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.934831</td>\n",
       "      <td>0.884478</td>\n",
       "      <td>0.894093</td>\n",
       "      <td>0.859607</td>\n",
       "      <td>0.911933</td>\n",
       "      <td>0.561281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision_square_mean</th>\n",
       "      <td>0.029770</td>\n",
       "      <td>-0.008769</td>\n",
       "      <td>0.934831</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.722816</td>\n",
       "      <td>0.808208</td>\n",
       "      <td>0.830003</td>\n",
       "      <td>0.812133</td>\n",
       "      <td>0.426537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cond_correlation_abs_mean</th>\n",
       "      <td>-0.052468</td>\n",
       "      <td>-0.021231</td>\n",
       "      <td>0.884478</td>\n",
       "      <td>0.722816</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.963535</td>\n",
       "      <td>0.770452</td>\n",
       "      <td>0.775812</td>\n",
       "      <td>0.639361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cond_correlation_square_mean</th>\n",
       "      <td>-0.006787</td>\n",
       "      <td>-0.009877</td>\n",
       "      <td>0.894093</td>\n",
       "      <td>0.808208</td>\n",
       "      <td>0.963535</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.798495</td>\n",
       "      <td>0.732240</td>\n",
       "      <td>0.587927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kld</th>\n",
       "      <td>-0.051600</td>\n",
       "      <td>-0.003765</td>\n",
       "      <td>0.859607</td>\n",
       "      <td>0.830003</td>\n",
       "      <td>0.770452</td>\n",
       "      <td>0.798495</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.705796</td>\n",
       "      <td>0.521010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iae</th>\n",
       "      <td>-0.064045</td>\n",
       "      <td>0.018798</td>\n",
       "      <td>0.911933</td>\n",
       "      <td>0.812133</td>\n",
       "      <td>0.775812</td>\n",
       "      <td>0.732240</td>\n",
       "      <td>0.705796</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.539679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dependence</th>\n",
       "      <td>0.006910</td>\n",
       "      <td>0.013820</td>\n",
       "      <td>0.561281</td>\n",
       "      <td>0.426537</td>\n",
       "      <td>0.639361</td>\n",
       "      <td>0.587927</td>\n",
       "      <td>0.521010</td>\n",
       "      <td>0.539679</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               var_row   var_col  precision_abs_mean  \\\n",
       "var_row                       1.000000  0.500000           -0.021973   \n",
       "var_col                       0.500000  1.000000            0.019489   \n",
       "precision_abs_mean           -0.021973  0.019489            1.000000   \n",
       "precision_square_mean         0.029770 -0.008769            0.934831   \n",
       "cond_correlation_abs_mean    -0.052468 -0.021231            0.884478   \n",
       "cond_correlation_square_mean -0.006787 -0.009877            0.894093   \n",
       "kld                          -0.051600 -0.003765            0.859607   \n",
       "iae                          -0.064045  0.018798            0.911933   \n",
       "dependence                    0.006910  0.013820            0.561281   \n",
       "\n",
       "                              precision_square_mean  \\\n",
       "var_row                                    0.029770   \n",
       "var_col                                   -0.008769   \n",
       "precision_abs_mean                         0.934831   \n",
       "precision_square_mean                      1.000000   \n",
       "cond_correlation_abs_mean                  0.722816   \n",
       "cond_correlation_square_mean               0.808208   \n",
       "kld                                        0.830003   \n",
       "iae                                        0.812133   \n",
       "dependence                                 0.426537   \n",
       "\n",
       "                              cond_correlation_abs_mean  \\\n",
       "var_row                                       -0.052468   \n",
       "var_col                                       -0.021231   \n",
       "precision_abs_mean                             0.884478   \n",
       "precision_square_mean                          0.722816   \n",
       "cond_correlation_abs_mean                      1.000000   \n",
       "cond_correlation_square_mean                   0.963535   \n",
       "kld                                            0.770452   \n",
       "iae                                            0.775812   \n",
       "dependence                                     0.639361   \n",
       "\n",
       "                              cond_correlation_square_mean       kld  \\\n",
       "var_row                                          -0.006787 -0.051600   \n",
       "var_col                                          -0.009877 -0.003765   \n",
       "precision_abs_mean                                0.894093  0.859607   \n",
       "precision_square_mean                             0.808208  0.830003   \n",
       "cond_correlation_abs_mean                         0.963535  0.770452   \n",
       "cond_correlation_square_mean                      1.000000  0.798495   \n",
       "kld                                               0.798495  1.000000   \n",
       "iae                                               0.732240  0.705796   \n",
       "dependence                                        0.587927  0.521010   \n",
       "\n",
       "                                   iae  dependence  \n",
       "var_row                      -0.064045    0.006910  \n",
       "var_col                       0.018798    0.013820  \n",
       "precision_abs_mean            0.911933    0.561281  \n",
       "precision_square_mean         0.812133    0.426537  \n",
       "cond_correlation_abs_mean     0.775812    0.639361  \n",
       "cond_correlation_square_mean  0.732240    0.587927  \n",
       "kld                           0.705796    0.521010  \n",
       "iae                           1.000000    0.539679  \n",
       "dependence                    0.539679    1.000000  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.corr(\"pearson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798e7e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC IAE                    : 0.8760504201680673\n",
      "AUC KLD                    : 0.8991596638655462\n",
      "AUC Conditional Correlation: 0.8907563025210083\n",
      "AUC Precision Matrix       : 0.8907563025210083\n"
     ]
    }
   ],
   "source": [
    "auc_iae = roc_auc_score(merged[\"dependence\"], merged[\"iae\"])\n",
    "auc_kld = roc_auc_score(merged[\"dependence\"], merged[\"kld\"])\n",
    "auc_corr = roc_auc_score(merged[\"dependence\"], merged[\"cond_correlation_abs_mean\"])\n",
    "auc_pmat = roc_auc_score(merged[\"dependence\"], merged[\"precision_abs_mean\"])\n",
    "print(\"AUC IAE                    :\",auc_iae)\n",
    "print(\"AUC KLD                    :\",auc_kld)\n",
    "print(\"AUC Conditional Correlation:\",auc_corr)\n",
    "print(\"AUC Precision Matrix       :\",auc_pmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b9b4cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fpr, tpr, thresholds = roc_curve(merged[\"dependence\"], merged[\"iae\"])\n",
    "plt.plot(fpr, tpr, label=\"iae\")\n",
    "fpr, tpr, thresholds = roc_curve(merged[\"dependence\"], merged[\"kld\"])\n",
    "plt.plot(fpr, tpr, label=\"kld\")\n",
    "fpr, tpr, thresholds = roc_curve(merged[\"dependence\"], merged[\"cond_correlation_abs_mean\"])\n",
    "plt.plot(fpr, tpr, label=\"cond_correlation_abs_mean\")\n",
    "fpr, tpr, thresholds = roc_curve(merged[\"dependence\"], merged[\"precision_abs_mean\"])\n",
    "plt.plot(fpr, tpr, label=\"precision_abs_mean\")\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd6cda5",
   "metadata": {},
   "source": [
    "#### Looking at the model splines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "149a1b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#_ = plot_splines(model.transformation,simulated_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8649bbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#_ = plot_splines(model.decorrelation_layers[0],simulated_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5aa64ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#_ = plot_splines(model.decorrelation_layers[1],simulated_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eb78f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#_ = plot_splines(model.decorrelation_layers[2],simulated_data_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mctm_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
